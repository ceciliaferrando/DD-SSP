import numpy as np
import pandas as pd
import json
import copy
from scipy import sparse

import sys

sys.path.append('..')

from mbi import Dataset
from dpsynth.workload import Workload
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from private_pgm_local.mechanisms import aim

from itertools import combinations
import pickle
import pdb

import os

import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
import argparse
from tqdm.auto import tqdm

from utils import *
from utils_LinReg import *



def public_method(X, y, X_test, y_test, cols_to_dummy, attribute_dict, one_hot):
    if one_hot:
        # feature encoding
        encoded_features = [col for col in X if col.split("_")[0] in cols_to_dummy]
        print(encoded_features)
        X_pub = normalize_minus1_1(X, attribute_dict, encoded_features)
        X_test_pub = normalize_minus1_1(X_test, attribute_dict, encoded_features)
        y_pub = normalize_minus1_1(y, attribute_dict, encoded_features)
    else:
        X_pub = copy.deepcopy(X)
        X_test_pub = copy.deepcopy(X_test)
        y_pub = copy.deepcopy(y)

    theta_public, mse_public, r2_public = public_linear_regression(X_pub, y_pub, X_test_pub, y_test)

    return (theta_public, mse_public, r2_public)


def get_aim_W_and_data(pgm_train_df, domain, target, marginals_pgm, epsilon, delta, model_size, max_iters, n_samples):
    pgm_dataset = Dataset(pgm_train_df, domain)
    mrgs = selectTargetMarginals(pgm_train_df.columns, target, mode=marginals_pgm)
    mrgs_wkld = Workload((mrg, sparse.identity) for mrg in mrgs)
    pgm_synth = PGMsynthesizer(pgm_dataset, epsilon, delta, mrgs_wkld, model_size, max_iters, n_samples)
    pgm_synth.aimsynthesizer()
    ans_wkld = pgm_synth.ans_wkld
    W = {key: ans_wkld[key].__dict__['values'] for key in ans_wkld}
    synth = pgm_synth.synth.df
    synth_X, synth_y = synth.loc[:, synth.columns != target], synth.loc[:, synth.columns == target]

    return W, synth_X, synth_y

def dp_query_ss_method(W_expanded, attribute_dict, training_columns, encoded_features, target, domain, n_train,
                              cols_to_dummy, one_hot, X_test, y_test):
    all_attributes_expanded = training_columns.append(pd.Index([target]))  # for aim dp query ss
    ZTZ = get_ZTZ(W_expanded, attribute_dict, all_attributes_expanded, cols_to_dummy, rescale=one_hot)
    XTX = ZTZ.loc[training_columns, training_columns]
    XTy = ZTZ.loc[training_columns, target]

    if one_hot:
        X_test = normalize_minus1_1(X_test, attribute_dict, encoded_features)
        y_test = normalize_minus1_1(y_test, attribute_dict, encoded_features)

    # get estimator
    theta_query_ss = np.linalg.solve(XTX, XTy)
    y_pred = np.dot(X_test, theta_query_ss)
    mse_query_ss = mean_squared_error(y_test, y_pred)
    r2_query_ss = r2_score(y_test, y_pred)

    return (theta_query_ss, mse_query_ss, r2_query_ss)


def dp_query_synth_data_method(synth_X, synth_y, training_columns, attribute_dict, cols_to_dummy, encoded_features, X_test, y_test, one_hot):
    # Here we handle the case in which we have to one-hot encode the columns
    if one_hot:
        # As above we drop the first column in Pandas to avoid multi-collinearity
        # issues in the train data. We then check whether there is a column with all 0 or all 1
        # and we remove it. The trick by which we remove the columns with the same value is to see
        # which columns have a standard deviation equal to 0 or not
        # Here we set drop_first = False because we don't know which level for the synthetic AIM data would
        # be the first. Since we are enforcing the columns to be the same as the training data above,
        # the filtering at the column level later should get rid of collinearity issues.
        synth_X_ohe = pd.get_dummies(synth_X, columns=cols_to_dummy, drop_first=False)
        synth_X_ohe.drop(synth_X_ohe.std()[synth_X_ohe.std() == 0].index, axis=1, inplace=True)

        # Now we only select the columns that are present in the train set
        synth_X_ohe = synth_X_ohe[[el for el in synth_X_ohe.columns if el in training_columns]]
        aim_columns = synth_X_ohe.columns

        # Now we also modify the X_test so that we are sure that X_test also has the same columns as
        # the synthetic version of the data generated by AIM.
        X_test_aim = add_and_subsets_cols_to_test_set(X_test, aim_columns)
        synth_X = synth_X_ohe.copy()
    else:
        X_test_aim = X_test

    synth_X_ordered = pd.DataFrame()

    for col in X_test_aim.columns:
        synth_X_ordered[col] = synth_X[col]

    if one_hot:
        synth_X_ordered = normalize_minus1_1(synth_X_ordered, attribute_dict, encoded_features)
        X_test_aim = normalize_minus1_1(X_test_aim, attribute_dict, encoded_features)

    theta_aim_synth, mse_aim_synth, r2_aim_synth = public_linear_regression(synth_X_ordered, synth_y,
                                                                            X_test_aim, y_test)

    return (theta_aim_synth, mse_aim_synth, r2_aim_synth)