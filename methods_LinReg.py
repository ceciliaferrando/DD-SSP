import numpy as np
import pandas as pd
import json
import copy
from scipy import sparse

import sys

sys.path.append('..')

from mbi import Dataset
from dpsynth.workload import Workload
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
from private_pgm_local.mechanisms import aim

from itertools import combinations
import pickle
import pdb

import os

import matplotlib.pyplot as plt
import seaborn as sns
from statsmodels.stats.outliers_influence import variance_inflation_factor
import argparse
from tqdm.auto import tqdm

from utils import *
from utils_LinReg import *



def public_method(X, y, X_test, y_test):

    theta_public, mse_public, r2_public = public_linear_regression(X, y, X_test, y_test)

    return (theta_public, mse_public, r2_public)


def get_aim_W_and_data(pgm_train_df, domain, target, marginals_pgm, epsilon, delta, model_size, max_iters, n_samples):
    pgm_dataset = Dataset(pgm_train_df, domain)
    mrgs = selectTargetMarginals(pgm_train_df.columns, target, mode=marginals_pgm)
    mrgs_wkld = Workload((mrg, sparse.identity) for mrg in mrgs)
    pgm_synth = PGMsynthesizer(pgm_dataset, epsilon, delta, mrgs_wkld, model_size, max_iters, n_samples)
    pgm_synth.aimsynthesizer()
    ans_wkld = pgm_synth.ans_wkld
    W = {key: ans_wkld[key].__dict__['values'] for key in ans_wkld}
    synth = pgm_synth.synth.df
    synth_X, synth_y = synth.loc[:, synth.columns != target], synth.loc[:, synth.columns == target]

    return W, synth_X, synth_y

def dp_query_ss_method(W_expanded, attribute_dict, training_columns, target,
                              cols_to_dummy, one_hot, X_test, y_test):
    all_attributes_expanded = training_columns.append(pd.Index([target]))  # for aim dp query ss
    ZTZ = get_ZTZ(W_expanded, attribute_dict, all_attributes_expanded, cols_to_dummy, rescale=one_hot)
    XTX = ZTZ.loc[training_columns, training_columns]
    XTy = ZTZ.loc[training_columns, target]

    print(XTX)
    print(ZTZ)

    for i,col in enumerate(XTX.columns):
        print(col, X_test.columns[i])
        assert col == X_test.columns[i]

    # get estimator
    theta_query_ss = np.linalg.solve(XTX, XTy)
    y_pred = np.dot(X_test, theta_query_ss)
    mse_query_ss = mean_squared_error(y_test, y_pred)
    r2_query_ss = r2_score(y_test, y_pred)

    return (theta_query_ss, mse_query_ss, r2_query_ss)


def dp_query_synth_data_method(synth_X, synth_y, training_columns, cols_to_dummy, attribute_dict, encoded_features,
                               X_test, y_test, one_hot):
    # Here we handle the case in which we have to one-hot encode the columns
    if one_hot:
        # As above we drop the first column in Pandas to avoid multi-collinearity
        # issues in the train data. We then check whether there is a column with all 0 or all 1
        # and we remove it. The trick by which we remove the columns with the same value is to see
        # which columns have a standard deviation equal to 0 or not
        # Here we set drop_first = False because we don't know which level for the synthetic AIM data would
        # be the first. Since we are enforcing the columns to be the same as the training data above,
        # the filtering at the column level later should get rid of collinearity issues.
        synth_X = pd.get_dummies(synth_X, columns=cols_to_dummy, drop_first=False)
        synth_X.drop(synth_X.std()[synth_X.std() == 0].index, axis=1, inplace=True)

        # Now we only select the columns that are present in the train set
        synth_X = synth_X[[el for el in synth_X.columns if el in training_columns]]

        # rescale to match the other methods
        synth_X = normalize_minus1_1(synth_X, attribute_dict, encoded_features)
        synth_y = normalize_minus1_1(synth_y, attribute_dict, encoded_features)

        aim_columns = synth_X.columns
        # Now we also modify the X_test so that we are sure that X_test also has the same columns as
        # the synthetic version of the data generated by AIM.
        X_test_aim = add_and_subsets_cols_to_test_set(X_test, aim_columns)
    else:
        X_test_aim = X_test

    synth_X_ordered = pd.DataFrame()

    for col in X_test_aim.columns:
        synth_X_ordered[col] = synth_X[col]

    for i,col in enumerate(synth_X_ordered.columns):
        assert col == X_test_aim.columns[i]

    theta_aim_synth, mse_aim_synth, r2_aim_synth = public_linear_regression(synth_X_ordered, synth_y,
                                                                            X_test_aim, y_test)

    return (theta_aim_synth, mse_aim_synth, r2_aim_synth)


def AdaSSP_linear_regression(X, y, epsilon, delta, rho, bound_X, bound_y, bound_XTX, X_test, y_test, original_y_range,
                             rescale):
    """Returns DP linear regression model and metrics using AdaSSP. AdaSSP is described in Algorithm 2 of
        https://arxiv.org/pdf/1803.02596.pdf.

    Args:
        X: df feature vectors
        y: df of labels
        epsilon: model needs to meet (epsilon, delta)-DP.
        delta: model needs to meet (epsilon, delta)-DP.
        rho: failure probability, default of 0.05 is from original paper
        bound_X, bound y: bounds on the L2 sensitivity
        bound_XTX: bound on the sensitivity of XTX (is data is one hot encoded, XTX is sparser, sensitivity must be adapted)
        X_test, y_test: test data for evaluation

    Returns:
        theta_dp: regression coefficients
        mse_dp: mean squared error
        r2_dp: r2 score
    """

    n, d = X.shape

    XTX = np.dot(X.T, X)
    XTy = np.dot(X.T, y).flatten()

    eigen_min = max(0, np.amin(np.linalg.eigvals(XTX)))
    z = np.random.normal(0, 1, size=1)
    sensitivity = np.sqrt(np.log(6 / delta)) / (epsilon / 3)
    eigen_min_dp = max(0,
                       eigen_min + sensitivity * (bound_XTX) * z -
                       (bound_XTX) * np.log(6 / delta) / (epsilon / 3))
    lambda_dp = max(0,
                    np.sqrt(d * np.log(6 / delta) * np.log(2 * (d ** 2) / rho)) * (bound_XTX) /
                    (epsilon / 3) - eigen_min_dp)

    tri = np.triu(np.random.normal(0, 1, (d, d)))
    Zsym = tri + tri.T - np.diag(np.diag(tri))
    XTX_dp = XTX + sensitivity * (bound_XTX) * Zsym
    print("epsilon =", epsilon, "XTX_dp noise variance", sensitivity * (bound_XTX))

    z = np.random.normal(0, 1, size=(d,))
    XTy_dp = XTy + sensitivity * bound_X * bound_y * z
    XTX_dp_reg = XTX_dp + lambda_dp * np.eye(d)

    theta_dp = np.linalg.solve(XTX_dp_reg, XTy_dp)

    y_pred = np.dot(X_test, theta_dp)

    # # scale y pred back to original domain if rescale needed. Linear rescaling.
    # if rescale:
    #     y_pred = (y_pred - (-1)) / (+1 - (-1)) * (original_y_range[1] - original_y_range[0]) + original_y_range[0]
    #     y_pred = y_pred.astype(int)
    mse_dp = mean_squared_error(y_test, y_pred)
    r2_dp = r2_score(y_test, y_pred)

    return theta_dp, mse_dp, r2_dp
