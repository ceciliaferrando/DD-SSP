{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaf09d55-95fa-4354-a0e7-7e533af38ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import os\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pdb\n",
    "import itertools\n",
    "import time\n",
    "import argparse\n",
    "import warnings\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, accuracy_score, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.special import softmax, expit\n",
    "from scipy.optimize import minimize, fmin_bfgs, fmin_tnc\n",
    "from scipy import sparse\n",
    "from absl import flags\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from private_pgm_local.src.mbi import Dataset, FactoredInference\n",
    "from diffprivlib_main.diffprivlib_local import models as dp\n",
    "from private_pgm_local.mechanisms import aim\n",
    "from dpsynth.workload import Workload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7a754-b680-4449-9860-07385230bdd7",
   "metadata": {},
   "source": [
    "UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c616a62-5234-4681-99ad-f23efed9d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, target_dict, n_limit, train_ratio, one_hot):\n",
    "    \"\"\"\n",
    "    Preprocesses the data for further analysis.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (str): Name of the dataset.\n",
    "        target_dict (dict): Dictionary containing target information for datasets.\n",
    "        n_limit (int): Limit on the number of data points.\n",
    "        train_ratio (float): Ratio of training data.\n",
    "        one_hot (bool): Whether to perform one-hot encoding.\n",
    "\n",
    "    Returns:\n",
    "        X (df): train features\n",
    "        X_test (df): test features\n",
    "        y (df): train target\n",
    "        y_test (df): test target\n",
    "        pgm_train_df (df): data for training AIM (AIM processes raw categorical tabular data, pre encoding)\n",
    "        domain (dict): attribute domain\n",
    "        target (str): name of the chosen predicted variable\n",
    "        attribute_dict (dict): attrinbute information in the form {'attr_name': [list of possible values]}\n",
    "        features_to_encode (list): columns to encode\n",
    "        encoded_features (list): list of feature names post encoding\n",
    "        original_ranges (dict): dictionary of the attribute ranges in the form {'attr_name': [min, max]}\n",
    "        training_columns (list): list of the features included in training phase\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the data\n",
    "    csv_path = '../hd-datasets-master/clean/' + dataset + '.csv'\n",
    "    meta_path = '../hd-datasets-master/clean/' + dataset + '-domain.json'\n",
    "    data = Dataset.load(csv_path, meta_path)  # for PGM\n",
    "    domain = data.domain\n",
    "    target = target_dict[dataset]\n",
    "\n",
    "    # Load the data\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Split the DF\n",
    "    if len(df) > n_limit:\n",
    "        df = df[:n_limit]\n",
    "    X, y, X_test, y_test = splitdf(df, target, train_ratio)\n",
    "    pgm_train_df = X.join(y)\n",
    "\n",
    "    # Create dictionary with attribute levels\n",
    "    attribute_dict = {}\n",
    "    for col in df:\n",
    "        unique_values = list(range(domain[col]))\n",
    "        attribute_dict[col] = unique_values\n",
    "    print(attribute_dict)\n",
    "\n",
    "    # If one_hot is active, then we one hot both the train set and the test set.\n",
    "    features_to_encode, training_columns = [], X.columns\n",
    "    \n",
    "    if one_hot:\n",
    "        print(f\"one-hot encoding {dataset}...\")\n",
    "        features_to_encode = get_features_to_encode(dataset)\n",
    "        X_ohe = pd.get_dummies(X, columns=features_to_encode, drop_first=True)\n",
    "        X_ohe.drop(X_ohe.std()[X_ohe.std() == 0].index, axis=1, inplace=True)\n",
    "        for col in X_ohe:\n",
    "            if col.endswith(\".0\"):\n",
    "                X_ohe.rename(columns={col: col.split(\".0\")[0]}, inplace=True)\n",
    "        training_columns = X_ohe.columns\n",
    "        X_test_ohe = pd.get_dummies(X_test, columns=features_to_encode, drop_first=True)\n",
    "        X_test_ohe = add_and_subsets_cols_to_test_set(X_test_ohe, training_columns)\n",
    "        for col in X_test_ohe:\n",
    "            if col.endswith(\".0\"):\n",
    "                X_test_ohe.rename(columns={col: col.split(\".0\")[0]}, inplace=True)\n",
    "        assert set(X_ohe.columns) == set(X_test_ohe.columns)\n",
    "        X = X_ohe.copy(deep=True)\n",
    "        X_test = X_test_ohe.copy()\n",
    "\n",
    "    encoded_features = [col for col in X if col.split(\"_\")[0] in features_to_encode]\n",
    "    original_ranges = {feature: [0, domain[feature]] for feature in attribute_dict.keys()}\n",
    "\n",
    "    if one_hot:\n",
    "        X = normalize_minus1_1(X, attribute_dict, encoded_features)\n",
    "        X_test = normalize_minus1_1(X_test, attribute_dict, encoded_features)\n",
    "        y = normalize_minus1_1(y, attribute_dict, encoded_features)\n",
    "        y_test = normalize_minus1_1(y_test, attribute_dict, encoded_features)\n",
    "\n",
    "    return X, X_test, y, y_test, pgm_train_df, domain, target, attribute_dict, features_to_encode, encoded_features, original_ranges, training_columns\n",
    "\n",
    "\n",
    "def splitdf(df, target, train_ratio):\n",
    "    \n",
    "    n = len(df)\n",
    "\n",
    "    idxs = np.array(range(n))\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    train_rows, test_rows = idxs[:int(train_ratio * n)], idxs[int(train_ratio * n):]\n",
    "    df_train, df_test = df.iloc[train_rows, :], df.iloc[test_rows, :]\n",
    "\n",
    "    df_X_train, df_y_train = df_train.loc[:, df_train.columns != target], df_train.loc[:, df_train.columns == target]\n",
    "    df_X_test, df_y_test = df_test.loc[:, df_test.columns != target], df_test.loc[:, df_test.columns == target]\n",
    "\n",
    "    return (df_X_train, df_y_train, df_X_test, df_y_test)\n",
    "\n",
    "\n",
    "def get_features_to_encode(dataset):\n",
    "    if dataset == \"adult\":\n",
    "        features_to_encode = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country']\n",
    "    elif dataset == \"ACSincome\" or \"ACSincome-LIN\":\n",
    "        features_to_encode = ['COW', 'MAR', 'RELP', 'RAC1P']\n",
    "    elif dataset == \"ACSemployment\":\n",
    "        features_to_encode = ['MAR', 'RELP', 'CIT', 'MIL', 'ANC', 'RAC1P']\n",
    "    elif dataset == \"ACSmobility\":\n",
    "        features_to_encode = ['MAR', 'CIT', 'MIL', 'ANC', 'RELP', 'RAC1P', 'GCL', 'COW', 'ESR']\n",
    "    elif dataset == \"ACSPublicCoverage\":\n",
    "        features_to_encode = ['MAR', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'ESR', 'FER', 'RAC1P']\n",
    "    elif dataset == \"ACSTravelTime\":\n",
    "        features_to_encode = ['MAR', 'ESP', 'MIG', 'RELP', 'RAC1P', 'CIT', 'OCCP', 'JWTR']\n",
    "    elif dataset == \"titanic\":\n",
    "        features_to_encode = ['Pclass', 'Cabin', 'Embarked']\n",
    "    return features_to_encode\n",
    "\n",
    "\n",
    "def add_and_subsets_cols_to_test_set(X_test, columns):\n",
    "    for col_val in columns:\n",
    "        if col_val not in X_test.columns:\n",
    "            X_test[col_val] = 0\n",
    "    return X_test[columns]\n",
    "\n",
    "\n",
    "def selectTargetMarginals(cols, target, mode='target-pairs'):\n",
    "    out = []\n",
    "    if mode == 'target-pairs':\n",
    "        for col in cols:\n",
    "            if col != target:\n",
    "                out.append((col, target))\n",
    "    elif mode == 'target-triplets':\n",
    "        cols_new = list(cols)\n",
    "        cols_new.remove(target)\n",
    "        tmp_pairs = combinations(cols_new, 2)\n",
    "        out = [(t[0], t[1], target) for t in tmp_pairs]\n",
    "    elif mode == 'target-pairs-target-triplets':\n",
    "        out = []\n",
    "        for col in cols:\n",
    "            if col != target:\n",
    "                out.append((col, target))\n",
    "        cols_new = list(cols)\n",
    "        cols_new.remove(target)\n",
    "        tmp_pairs = combinations(cols_new, 2)\n",
    "        out.extend([(t[0], t[1], target) for t in tmp_pairs])\n",
    "    elif mode == \"all-pairs\":\n",
    "        out = list(combinations(cols, 2))\n",
    "    elif mode == 'all-triplets':\n",
    "        out = list(combinations(cols, 3))\n",
    "    elif mode == \"no-target-pairs\":\n",
    "        cols_new = list(cols)\n",
    "        cols_new.remove(target)\n",
    "        out = combinations(cols_new, 2)\n",
    "    return out\n",
    "\n",
    "def get_bound_XTX(attribute_dict, target, features_to_encode, one_hot):\n",
    "\n",
    "    if not one_hot: # then data is binary synthetic data\n",
    "        bound_X = np.sqrt(np.sum([max(attribute_dict[f])**2 for f in attribute_dict if f!=target]))\n",
    "        bound_XTX = bound_X**2\n",
    "        \n",
    "    elif one_hot: # follow sensitivity computation as described in paper\n",
    "        bound_XTX = len(attribute_dict.keys()) - 1    #excludes target\n",
    "        bound_X = np.sqrt(bound_XTX)\n",
    "\n",
    "    return bound_XTX, bound_X\n",
    "\n",
    "def normalize_minus1_1(X, attribute_dict, encoded_features):\n",
    "    original_ranges = {attr: [min(attribute_dict[attr]), max(attribute_dict[attr])]\n",
    "                       for attr in attribute_dict.keys()}\n",
    "\n",
    "    X_out = pd.DataFrame()\n",
    "\n",
    "    for col in X.columns:\n",
    "\n",
    "        # this is in case the test set has columns of zeros\n",
    "        if len(set(X[col])) == 1:\n",
    "            X_out[col] = X[col]\n",
    "\n",
    "        # if the column corresponds to a categorical feature that has been one-hot encoded, keep it in domain [0, 1]\n",
    "        if col in encoded_features:\n",
    "            X_out[col] = X[col]\n",
    "\n",
    "        # for all other features, rescale to [-1, 1]\n",
    "        else:\n",
    "            colmin = original_ranges[col][0]\n",
    "            colmax = original_ranges[col][1]\n",
    "\n",
    "            col_1s = (1 - (-1)) * ((X[col] - colmin) / (colmax - colmin)) - 1\n",
    "            X_out[col] = col_1s\n",
    "\n",
    "    return X_out\n",
    "\n",
    "\n",
    "# AIM Utils ############################################################################################################\n",
    "\n",
    "class PGMsynthesizer():\n",
    "\n",
    "    # this class is a wrapper for constructing the AIM model\n",
    "\n",
    "    def __init__(self, data, epsilon, delta, measurements, model_size, max_iters, num_synth_rows):\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.data = data\n",
    "        self.measurements = measurements\n",
    "        self.model_size = model_size\n",
    "        self.max_iters = max_iters\n",
    "        self.num_synth_rows = num_synth_rows\n",
    "        self.aim_model = None\n",
    "        self.synth = None\n",
    "        self.G = None\n",
    "        self.ans_wkld = None\n",
    "\n",
    "    def mstsynthesizer(self):\n",
    "        self.synth, self.G = mst.mst(self.data, self.epsilon, self.delta)\n",
    "\n",
    "    def mwemsynthesizer(self):\n",
    "        self.synth, self.G = mwem.mwem_pgm(self.data, self.epsilon, workload=self.measurements)\n",
    "\n",
    "    def v13synthesizer(self):\n",
    "        v13model = v13.V13(self.epsilon, self.delta)\n",
    "        self.synth, self.G = v13model.run(self.data, self.measurements)\n",
    "\n",
    "    def aimsynthesizer(self):\n",
    "        aim_model = aim.AIM(epsilon=self.epsilon, delta=self.delta, max_iters=self.max_iters,\n",
    "                           max_model_size=self.model_size)\n",
    "        self.synth, self.G, self.ans_wkld = aim_model.run(self.data, self.measurements, self.num_synth_rows,\n",
    "                                                         output_graphical_model=True)\n",
    "\n",
    "\n",
    "# DQ Query Approximation Utils #########################################################################################\n",
    "\n",
    "def expand_W(W, attribute_dict):\n",
    "    # add symmetric entries\n",
    "    W_expanded = copy.deepcopy(W)\n",
    "    for el in W:\n",
    "        W_expanded[el[1], el[0]] = W[el].T\n",
    "\n",
    "    # add (x, x) pairs\n",
    "    for col in attribute_dict.keys():\n",
    "        table_with_col = [W_expanded[tple] for tple in W_expanded if tple[0] == col][0]\n",
    "        col_counts = np.sum(table_with_col, axis=1)\n",
    "        W_expanded[col, col] = np.diag(col_counts)\n",
    "\n",
    "    return W_expanded\n",
    "\n",
    "class Chebyshev:\n",
    "    \"\"\"\n",
    "    Chebyshev(a, b, n, func)\n",
    "    Given a function func, lower and upper limits of the interval [a,b],\n",
    "    and maximum degree n, this class computes a Chebyshev approximation\n",
    "    of the function.\n",
    "    Method eval(x) yields the approximated function value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a, b, n, func):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.func = func\n",
    "\n",
    "        bma = 0.5 * (b - a)\n",
    "        bpa = 0.5 * (b + a)\n",
    "        f = [func(math.cos(math.pi * (k + 0.5) / n) * bma + bpa) for k in range(n)]\n",
    "        fac = 2.0 / n\n",
    "        self.c = [fac * sum([f[k] * math.cos(math.pi * j * (k + 0.5) / n)\n",
    "                             for k in range(n)]) for j in range(n)]\n",
    "\n",
    "    def eval(self, x):\n",
    "        a, b = self.a, self.b\n",
    "        y = (2.0 * x - a - b) * (1.0 / (b - a))\n",
    "        y2 = 2.0 * y\n",
    "        (d, dd) = (self.c[-1], 0)  # Special case first step for efficiency\n",
    "        for cj in self.c[-2:0:-1]:  # Clenshaw's recurrence\n",
    "            (d, dd) = (y2 * d - dd + cj, d)\n",
    "        return y * d - dd + 0.5 * self.c[0]  # Last step is different\n",
    "\n",
    "\n",
    "def phi_logit(x):\n",
    "    return -math.log(1 + math.exp(-x))\n",
    "\n",
    "def logit_2(x):\n",
    "    return math.log(1 + math.exp(x))\n",
    "\n",
    "\n",
    "def get_ZTZ(W, attribute_dict, columns, features_to_encode, rescale):\n",
    "    \"\"\"\n",
    "        W: [dict] marginal tables in the form {(\"feature_A\", \"feature_B\"); m_A x m_B np.array of counts, ...}\n",
    "        attribute_dict: [dict] attribute levels, {\"attr_A\": list of ordered possible levels for attr_A, ...}\n",
    "                                - should include target\n",
    "        columns: [list] list of names of post-encoding attributes\n",
    "        features_to_encode: [list] list of features requiring 1-hot encoding}\n",
    "        rescale: [boolean] True if rescaling numerical non binary attributes in [-1, 1], TBC\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize ZTZ as a DataFrame with *named* columns and rows\n",
    "    base_matrix = np.zeros((len(columns), len(columns)))\n",
    "    ZTZ = pd.DataFrame(base_matrix, columns=columns, index=columns)\n",
    "\n",
    "    # loop through attribute pairs\n",
    "\n",
    "    for a, attr_a in enumerate(columns):\n",
    "        for b, attr_b in enumerate(columns[a:]):\n",
    "\n",
    "            # root name of the attributes\n",
    "            attr_a_orig = attr_a.split(\"_\")[0]\n",
    "            attr_b_orig = attr_b.split(\"_\")[0]\n",
    "            # possible level values\n",
    "            a_values = attribute_dict[attr_a_orig]\n",
    "            b_values = attribute_dict[attr_b_orig]\n",
    "            if rescale:\n",
    "                if attr_a_orig not in features_to_encode:\n",
    "                    a_range_min, a_range_max = min(a_values), max(a_values)\n",
    "                    a_values = [(1 -(-1)) * ((val - a_range_min) / (a_range_max - a_range_min)) - 1 for val in a_values]\n",
    "                if attr_b_orig not in features_to_encode:\n",
    "                    b_range_min, b_range_max = min(b_values), max(b_values)\n",
    "                    b_values = [(1 -(-1)) * ((val - b_range_min) / (b_range_max - b_range_min)) - 1 for val in b_values]\n",
    "\n",
    "            # case 1: a and b are both ordinal\n",
    "            if attr_a_orig not in features_to_encode and attr_b_orig not in features_to_encode:\n",
    "                mu_ab = W[(attr_a_orig, attr_b_orig)]\n",
    "                for j, j_value in enumerate(a_values):\n",
    "                    for k, k_value in enumerate(b_values):\n",
    "                        ZTZ[attr_a][attr_b] += j_value * k_value * mu_ab[j, k]\n",
    "\n",
    "            # case 2.1: a is ordinal, b is encoded\n",
    "            elif attr_a_orig not in features_to_encode and attr_b_orig in features_to_encode:\n",
    "                mu_ab = W[(attr_a_orig, attr_b_orig)]\n",
    "                t = int(float(attr_b.split(\"_\")[-1]))  # get level number ***** ASSUMES LEVELS CORRESPOND TO THE NAMES ******\n",
    "                ZTZ[attr_a][attr_b] = np.sum(np.multiply(mu_ab[:, t], a_values))\n",
    "\n",
    "            # case 2.2: a is encoded, b is ordinal\n",
    "            elif attr_a_orig in features_to_encode and attr_b_orig not in features_to_encode:\n",
    "                mu_ab = W[(attr_a_orig, attr_b_orig)]\n",
    "                s = int(float(attr_a.split(\"_\")[-1]))  # get level number ***** ASSUMES LEVELS CORRESPOND TO THE NAMES ******\n",
    "                ZTZ[attr_a][attr_b] = np.sum(np.multiply(mu_ab[s, :], b_values))\n",
    "\n",
    "            # case 3: a and b are both encoded\n",
    "            elif attr_a_orig in features_to_encode and attr_b_orig in features_to_encode:\n",
    "                s = int(float(attr_a.split(\"_\")[-1]))  # get level number ***** ASSUMES LEVELS CORRESPOND TO THE NAMES ******\n",
    "                t = int(float(attr_b.split(\"_\")[-1])) # get level number ***** ASSUMES LEVELS CORRESPOND TO THE NAMES ******\n",
    "                mu_ab = W[(attr_a_orig, attr_b_orig)]\n",
    "                ZTZ[attr_a][attr_b] = mu_ab[s, t]\n",
    "\n",
    "    # copy lower tri to upper tri\n",
    "    ZTZ = ZTZ + ZTZ.T - np.diag(np.diag(ZTZ))\n",
    "\n",
    "    return ZTZ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51137f-13d1-4b1a-8af3-29271f931207",
   "metadata": {},
   "source": [
    "LOGREG UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bc49a94-4f9b-40f9-9b5d-fe1c2f15ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionObjective():\n",
    "    @staticmethod\n",
    "    def sigmoid_v2(x, theta):\n",
    "        z = np.dot(x, theta)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def hypothesis(theta, x):\n",
    "        return LogisticRegressionObjective.sigmoid_v2(x, theta)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        h = LogisticRegressionObjective.hypothesis(theta, x)\n",
    "        return -(1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        h = LogisticRegressionObjective.hypothesis(theta, x)\n",
    "        return (1 / m) * np.dot(x.T, (h - y))\n",
    "\n",
    "def genobjpert_get_params(X, epsilon, delta, lmda, zeta):\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    delta_thrs = 2*lmda/epsilon\n",
    "    Delta = delta_thrs\n",
    "    b_var = zeta**2 * (8 * np.log(2/delta) + 4*epsilon) / (epsilon**2) * np.eye(d)\n",
    "    b = np.random.multivariate_normal(np.zeros((b_var.shape[0], )), b_var)\n",
    "\n",
    "    return Delta, b\n",
    "\n",
    "def dp_objective(theta, X, y, n, d, Delta, b):\n",
    "\n",
    "    base_loss = LogisticRegressionObjective.loss(theta, X, y)\n",
    "    regularizer = 1/n * 0   #assumed zero\n",
    "    sec_term = Delta/(2*n) * np.dot(theta.T, theta)\n",
    "    third_term = np.dot(b.T, theta)/n\n",
    "\n",
    "    return base_loss + regularizer + sec_term + third_term\n",
    "\n",
    "def dp_gradient(theta, X, y, n, d, Delta, b):\n",
    "    base_gradient = LogisticRegressionObjective.gradient(theta, X, y)\n",
    "\n",
    "    reg_term = 1/n * np.zeros((d,))  # Assumed zero\n",
    "    second_term = Delta/n * theta\n",
    "    third_term = b/n\n",
    "\n",
    "    return base_gradient + reg_term + second_term + third_term\n",
    "\n",
    "def get_dp_approx_ll(theta, yTX, XTXy2, a, b, c, n):\n",
    "    dp_approx_ll = n * a + np.dot(theta, yTX) * b + np.dot(np.dot(theta, XTXy2), theta) * c\n",
    "    return dp_approx_ll\n",
    "\n",
    "class SSApproxLL():\n",
    "\n",
    "    def __init__(self, ch, yTX, XTXy2, n, penalty, alpha):\n",
    "        self.n = n\n",
    "        self.ch = ch\n",
    "        self.penalty = penalty\n",
    "        self.alpha = alpha\n",
    "        self.theta = None\n",
    "        self.yTX = yTX\n",
    "        self.XTXy2 = XTXy2\n",
    "\n",
    "    def fit(self):\n",
    "        self.optimize()\n",
    "        return self\n",
    "\n",
    "    def log_likelihood(self, theta):\n",
    "        a, b, c = self.ch.c\n",
    "        term = get_dp_approx_ll(theta, self.yTX, self.XTXy2, a, b, c, self.n)\n",
    "        term = 1 / self.n * term\n",
    "        return term\n",
    "\n",
    "    def optimize(self):\n",
    "\n",
    "        def l2_penalty(theta):\n",
    "            return np.sum(theta ** 2)\n",
    "\n",
    "        x0 = [.0] * len(self.yTX)\n",
    "\n",
    "        if self.penalty == None:\n",
    "            res = minimize(lambda theta: -self.log_likelihood(theta),\n",
    "                           x0,\n",
    "                           method='L-BFGS-B',\n",
    "                           options={'maxiter': 10000},\n",
    "                           tol=0.00001)\n",
    "            theta_star = res.x\n",
    "            fun = res.fun\n",
    "\n",
    "        elif self.penalty == 'l2':\n",
    "            res = minimize(lambda theta: -self.log_likelihood(theta) + self.alpha * l2_penalty(theta),\n",
    "                           x0,\n",
    "                           method='L-BFGS-B',\n",
    "                           options={'maxiter': 10000},\n",
    "                           tol=0.00001)\n",
    "            theta_star = res.x\n",
    "            fun = res.fun - self.alpha * l2_penalty(theta_star)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Unknown penalty type, choose None or l2')\n",
    "\n",
    "        self.theta = theta_star\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = np.dot(X, self.theta.T)\n",
    "        y_pred_proba = expit(z)\n",
    "        return (y_pred_proba)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        z = np.dot(X, self.theta.T)\n",
    "        y_pred_proba = expit(z)\n",
    "        y_pred = 2 * ((y_pred_proba >= threshold).astype(int)) - 1\n",
    "        print(\"y pred dpapproxss\", y_pred)\n",
    "        return (y_pred)\n",
    "\n",
    "def get_aim_model(pgm_train_df, domain, target, marginals_pgm, epsilon, delta, model_size, max_iters, n_samples):\n",
    "    pgm_dataset = Dataset(pgm_train_df, domain)\n",
    "    mrgs = selectTargetMarginals(pgm_train_df.columns, target, mode=marginals_pgm)\n",
    "    mrgs_wkld = Workload((mrg, sparse.identity) for mrg in mrgs)\n",
    "    pgm = PGMsynthesizer(pgm_dataset, epsilon, delta, mrgs_wkld, model_size, max_iters, n_samples)\n",
    "    pgm.aimsynthesizer()\n",
    "    return pgm, mrgs\n",
    "\n",
    "def testLogReg(theta, X_test, y_test):\n",
    "    logits = np.dot(X_test, theta)\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "    auc = roc_auc_score(y_test, probabilities)\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b768b927-f229-4d48-ad2f-87a2eb73091a",
   "metadata": {},
   "source": [
    "LOGRER METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1420179-8642-4d0a-bf55-a7990d151f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def public_logreg(X, y):\n",
    "    model = LogisticRegression(penalty=None, fit_intercept=False, max_iter=1000)\n",
    "    model.fit(X.to_numpy(), y.to_numpy().ravel())\n",
    "    theta = model.coef_.ravel()   # probabilities for class 1\n",
    "    theta = pd.DataFrame(theta)\n",
    "    theta.set_index(X.columns, inplace=True)\n",
    "    return theta\n",
    "\n",
    "def dp_query_approx_ss_logreg(W_expanded, attribute_dict, training_columns, encoded_features, target, domain, n,\n",
    "                              features_to_encode, one_hot, cheb, phi_logit, C=1.0):\n",
    "\n",
    "    all_attributes_expanded = training_columns.append(pd.Index([target]))\n",
    "    ZTZ = get_ZTZ(W_expanded, attribute_dict, all_attributes_expanded, features_to_encode, rescale=True)\n",
    "    XTXy2 = ZTZ.loc[training_columns, training_columns]\n",
    "    XTy = ZTZ.loc[training_columns, target]\n",
    "    alpha = 1 / (n * C)\n",
    "    model = SSApproxLL(cheb, XTy, XTXy2, n, penalty=None, alpha=alpha)\n",
    "    model.fit()\n",
    "    theta = model.theta\n",
    "\n",
    "    return theta\n",
    "\n",
    "def objective_perturbation_method(X, y, attribute_dict, target, features_to_encode, epsilon, delta, one_hot):\n",
    "\n",
    "    n, d = X.shape\n",
    "    # maxvals = np.array(np.amax(X.to_numpy(), axis = 0)).reshape(-1,1)\n",
    "    # max_row_norm = np.sqrt(np.sum([a**2 for a in maxvals]))\n",
    "    if one_hot:\n",
    "        max_row_norm = np.sqrt(len(attribute_dict.keys())-1)   # excludes target\n",
    "    else:\n",
    "        _, max_row_norm = get_bound_XTX(attribute_dict, target, features_to_encode, one_hot)\n",
    "    bound_y = np.abs(max(attribute_dict[target]))\n",
    "\n",
    "    zeta = max_row_norm\n",
    "    lmda = smooth_const = max_row_norm ** 2 / 4\n",
    "\n",
    "    def sigmoid_v2(x, theta):\n",
    "        z = np.dot(x, theta)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def hypothesis(theta, x):\n",
    "        return sigmoid_v2(x, theta)\n",
    "\n",
    "    def cost_function(theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        h = hypothesis(theta, x)\n",
    "        return -(1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "\n",
    "    def gradient_fun(theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        h = hypothesis(theta, x)\n",
    "        return (1 / m) * np.dot(x.T, (h - y))\n",
    "\n",
    "    # Run the object perturbation\n",
    "    Delta, b = genobjpert_get_params(X.to_numpy(), epsilon, delta, lmda, zeta)\n",
    "\n",
    "    # Run the iteration\n",
    "    # fmin_tnc returns the convergence message as third argument in its output\n",
    "    # 0 means local minimium reached, 1 and 2 means convergence by function value or theta value\n",
    "    # 3 is maximum number of iterations reached, 4 linear search failed\n",
    "    finish_opt = False\n",
    "    patience = 3\n",
    "\n",
    "    while not finish_opt and patience > 0:\n",
    "\n",
    "        theta0 = np.random.normal(loc=0, scale=0.01, size=X.shape[1]).reshape(-1, )\n",
    "        theta_opt = fmin_tnc(func=dp_objective, x0=theta0, fprime=dp_gradient, maxfun=10000, disp=0,\n",
    "                             args=(X.to_numpy(), y.to_numpy().reshape(-1, ), n, d, Delta, b))\n",
    "\n",
    "        theta_final, n_it_run, final_message = theta_opt\n",
    "\n",
    "        if final_message in [0, 1, 2]:\n",
    "            finish_opt = True\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "    return theta_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787af8c-d508-4395-bab8-2b0d4e825980",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89b14324-db8f-404e-bf80-02ac8799f1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], 'workclass': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'fnlwgt': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], 'education': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 'education-num': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15], 'marital-status': [0, 1, 2, 3, 4, 5, 6], 'occupation': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14], 'relationship': [0, 1, 2, 3, 4, 5], 'race': [0, 1, 2, 3, 4], 'sex': [0, 1], 'capital-gain': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], 'capital-loss': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], 'hours-per-week': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31], 'native-country': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41], 'income>50K': [0, 1]}\n",
      "one-hot encoding adult...\n",
      "X.shape (14000, 100)\n",
      "X_test.shape (6000, 100)\n",
      "y.shape (14000, 1)\n",
      "y_test.shape (6000, 1)\n"
     ]
    }
   ],
   "source": [
    "dataset = 'adult'\n",
    "csv_path = '../hd-datasets-master/clean/' + dataset + '.csv'\n",
    "meta_path = '../hd-datasets-master/clean/' + dataset + '-domain.json'\n",
    "epsilon = 0.1\n",
    "delta = 1e-5\n",
    "n_limit = 20_000\n",
    "train_ratio = 0.7\n",
    "one_hot = True\n",
    "\n",
    "# AIM model parameters\n",
    "model_size = 100  \n",
    "max_iters = 1000  \n",
    "PGMmarginals = 'all-pairs'\n",
    "\n",
    "target_dict = {'adult': 'income>50K', 'titanic': 'Survived', 'diabetes': 'Outcome',\n",
    "               'ACSemployment': 'ESR', 'ACSincome': 'PINCP', \"ACSmobility\": 'MIG',\n",
    "               \"ACSPublicCoverage\": 'PUBCOV', 'ACSTravelTime': 'JWMNP', 'logregbinary10': 'predicted',\n",
    "               'logregbinary5lowcorr': 'predicted', 'logregbinary7lowcorr': 'predicted',\n",
    "               'logregbinary10lowcorr': 'predicted', 'logregbinary20lowcorr': 'predicted',\n",
    "               'logregbinary30lowcorr': 'predicted'}\n",
    "\n",
    "np.random.seed(237)\n",
    "\n",
    "(X, X_test, y, y_test, \n",
    " pgm_train_df, domain, target, \n",
    " attribute_dict, features_to_encode, encoded_features, \n",
    " original_ranges, training_columns) = preprocess_data(dataset, target_dict, n_limit, train_ratio, one_hot)\n",
    "\n",
    "n, d = X.shape\n",
    "\n",
    "print(f\"X.shape {X.shape}\")\n",
    "print(f\"X_test.shape {X_test.shape}\")\n",
    "print(f\"y.shape {y.shape}\")\n",
    "print(f\"y_test.shape {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0e3cf-1bb9-4767-9026-6c0853b5928e",
   "metadata": {},
   "source": [
    "PUBLIC LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eebe4e46-8625-49dd-b6c3-917cadc00a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_public = public_logreg(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74681638-cc85-4a21-b2d4-3931774598ba",
   "metadata": {},
   "source": [
    "TRAIN AND SAVE AIM MODEL, OBTAIN MARGINAL TABLES AND SYNTHETIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccd9058-5188-4140-8e82-3353b543fd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Sigma 554.9176837222757\n",
      "(!!!!!!!!!!!!!!!!!!!!!!) Reducing sigma 277.45884186113784\n"
     ]
    }
   ],
   "source": [
    "# 1) get AIM model and save it\n",
    "aim_model, workload = get_aim_model(pgm_train_df, domain, target, PGMmarginals, epsilon, delta, model_size, max_iters, len(X))\n",
    "aim_model_graph = aim_model.G\n",
    "with open('aim_model.pkl', 'wb') as f:\n",
    "    dill.dump(aim_model_graph, f)\n",
    "\n",
    "# 2) load AIM model and get marginal tables and synthetic data X_synth, y_synth\n",
    "with open('aim_model.pkl', 'rb') as f:\n",
    "    aim_model_graph = dill.load(f)\n",
    "\n",
    "aim_ans_wkld = {cl: aim_model_graph.project(cl) for cl in workload}\n",
    "W = {key: aim_ans_wkld[key].__dict__['values'] for key in aim_ans_wkld}\n",
    "\n",
    "synth = aim_model_graph.synthetic_data(rows=n).df\n",
    "synth_X, synth_y = synth.loc[:, synth.columns != target], synth.loc[:, synth.columns == target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918fcc99-6994-4cca-ac17-bc1db84da5c0",
   "metadata": {},
   "source": [
    "DPQUERYSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb9f443-4f5b-43b3-8f8e-a93615fb1355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W_expanded = expand_W(W, attribute_dict)\n",
    "train_columns = X.columns\n",
    "\n",
    "# get Chebyshev\n",
    "r = 6\n",
    "degree = 3\n",
    "C = 1.0\n",
    "cheb = Chebyshev(-r, r, degree, phi_logit)\n",
    "\n",
    "# approximate sufficient statistics\n",
    "theta_dpqueryss = dp_query_approx_ss_logreg(W_expanded, attribute_dict, training_columns, \n",
    "                                            encoded_features, target, domain, n,\n",
    "                                            features_to_encode, one_hot, cheb, phi_logit, C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee2b5b-1059-48b4-b4cc-bcd7a3a3d5a6",
   "metadata": {},
   "source": [
    "DP AIM SYNTHETIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c0c6aa-28da-4154-8472-c77c234c5abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_X_ohe = pd.get_dummies(synth_X, columns=features_to_encode, drop_first=True)\n",
    "\n",
    "missing_cols = []\n",
    "for col in X.columns:\n",
    "    if col not in synth_X_ohe.columns:\n",
    "        missing_cols.append(col)\n",
    "\n",
    "theta_aimsynth = public_logreg(synth_X_ohe, synth_y).T\n",
    "for col in missing_cols:\n",
    "    theta_aimsynth[col] = 0\n",
    "\n",
    "theta_aimsynth = theta_aimsynth.reindex(columns=X.columns, fill_value=0)\n",
    "\n",
    "for i,col in enumerate(X.columns):\n",
    "    assert col == theta_aimsynth.columns[i] \n",
    "\n",
    "theta_aimsynth = theta_aimsynth.T.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9934b3-5cd0-40f1-9817-8394920c9540",
   "metadata": {},
   "source": [
    "DP OBJECTIVE PERTURBATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d81259-8d76-4be0-84c2-02f91023ab39",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta_objpert = objective_perturbation_method(X, y, attribute_dict, target, features_to_encode, epsilon, delta, one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e61e54-6c5b-434d-bb23-01298c46df87",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f41152c-c62c-46ea-8848-9a997e696f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_public = testLogReg(theta_public, X_test, y_test)\n",
    "auc_dpqueryss = testLogReg(theta_dpqueryss, X_test, y_test)\n",
    "auc_aimsynth = testLogReg(theta_aimsynth, X_test, y_test)\n",
    "auc_aimsynth = testLogReg(theta_aimsynth, X_test, y_test)\n",
    "auc_objpert = testLogReg(theta_objpert, X_test, y_test)\n",
    "\n",
    "print(f\"public auc: {auc_public}\")\n",
    "print(f\"dpqueryss auc: {auc_dpqueryss}\")\n",
    "print(f\"aimsynth auc: {auc_aimsynth}\")\n",
    "print(f\"objpert auc: {auc_objpert}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e1b340-173e-4a99-af37-5af9d0a5a9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpsynthesisml-pyenv310",
   "language": "python",
   "name": "dpsynthesisml-pyenv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
