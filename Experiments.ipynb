{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aaf09d55-95fa-4354-a0e7-7e533af38ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "import os\n",
    "import argparse\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pdb\n",
    "import itertools\n",
    "import time\n",
    "import argparse\n",
    "import warnings\n",
    "import pickle\n",
    "import dill\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, accuracy_score, confusion_matrix, roc_auc_score, mean_squared_error\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.special import softmax, expit\n",
    "from scipy.optimize import minimize, fmin_bfgs, fmin_tnc\n",
    "from scipy import sparse\n",
    "from absl import flags\n",
    "from datetime import datetime\n",
    "from itertools import combinations\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from private_pgm_local.src.mbi import Dataset, FactoredInference\n",
    "from diffprivlib_main.diffprivlib_local import models as dp\n",
    "from private_pgm_local.mechanisms import aim\n",
    "from dpsynth.workload import Workload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff7a754-b680-4449-9860-07385230bdd7",
   "metadata": {},
   "source": [
    "## UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c616a62-5234-4681-99ad-f23efed9d797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(dataset, target_dict, n_limit, train_ratio, one_hot):\n",
    "    \"\"\"\n",
    "    Preprocesses the data for further analysis.\n",
    "\n",
    "    Parameters:\n",
    "        dataset (str): Name of the dataset.\n",
    "        target_dict (dict): Dictionary containing target information for datasets.\n",
    "        n_limit (int): Limit on the number of data points.\n",
    "        train_ratio (float): Ratio of training data.\n",
    "        one_hot (bool): Whether to perform one-hot encoding.\n",
    "\n",
    "    Returns:\n",
    "        X (df): train features\n",
    "        X_test (df): test features\n",
    "        y (df): train target\n",
    "        y_test (df): test target\n",
    "        pgm_train_df (df): data for training AIM (AIM processes raw categorical tabular data, pre encoding)\n",
    "        domain (dict): attribute domain\n",
    "        target (str): name of the chosen predicted variable\n",
    "        attribute_dict (dict): attrinbute information in the form {'attr_name': [list of possible values]}\n",
    "        features_to_encode (list): columns to encode\n",
    "        encoded_features (list): list of feature names post encoding\n",
    "        original_ranges (dict): dictionary of the attribute ranges in the form {'attr_name': [min, max]}\n",
    "        all_columns (list): list of the features included in training phase\n",
    "    \"\"\"\n",
    "\n",
    "    # Import the data\n",
    "    csv_path = '../hd-datasets-master/clean/' + dataset + '.csv'\n",
    "    meta_path = '../hd-datasets-master/clean/' + dataset + '-domain.json'\n",
    "    data = Dataset.load(csv_path, meta_path)  # for PGM\n",
    "    domain = data.domain\n",
    "    target = target_dict[dataset]\n",
    "\n",
    "    # Load the data\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    all_columns = df.columns\n",
    "\n",
    "    # Split the DF\n",
    "    if len(df) > n_limit:\n",
    "        df = df[:n_limit]\n",
    "    X, y, X_test, y_test = splitdf(df, target, train_ratio)\n",
    "    pgm_train_df = X.join(y)\n",
    "\n",
    "    # Create dictionary with attribute levels\n",
    "    attribute_dict = {}\n",
    "    for col in df:\n",
    "        unique_values = list(range(domain[col]))\n",
    "        attribute_dict[col] = unique_values\n",
    "    print(attribute_dict)\n",
    "\n",
    "    # If one_hot is active, then we one hot both the train set and the test set.\n",
    "    features_to_encode, all_columns = [], X.columns\n",
    "    \n",
    "    if one_hot:\n",
    "        print(f\"one-hot encoding {dataset}...\")\n",
    "        features_to_encode = get_features_to_encode(dataset)\n",
    "        X_ohe = one_hot_encode(X, features_to_encode, attribute_dict)\n",
    "        X_test_ohe = one_hot_encode(X_test, features_to_encode, attribute_dict)\n",
    "        assert set(X_ohe.columns) == set(X_test_ohe.columns)\n",
    "        X = X_ohe.copy(deep=True)\n",
    "        X_test = X_test_ohe.copy()\n",
    "\n",
    "    def Union(lst1, lst2):\n",
    "        final_list = list(set(lst1) | set(lst2))\n",
    "        return final_list\n",
    "\n",
    "    all_columns = pd.Index(Union(X.columns, X_test.columns))\n",
    "    encoded_features = [col for col in X if col.split(\"_\")[0] in features_to_encode]\n",
    "    original_ranges = {feature: [0, domain[feature]] for feature in attribute_dict.keys()}\n",
    "\n",
    "    if one_hot:\n",
    "        X = normalize_minus1_1(X, attribute_dict, encoded_features)\n",
    "        X_test = normalize_minus1_1(X_test, attribute_dict, encoded_features)\n",
    "        y = normalize_minus1_1(y, attribute_dict, encoded_features)\n",
    "        y_test = normalize_minus1_1(y_test, attribute_dict, encoded_features)\n",
    "\n",
    "    zero_std_cols = []\n",
    "    for col in X.columns:\n",
    "        if np.std(X[col]) == 0:\n",
    "            print(f\"feature {col} is a zero vector! Dropping it from training dataset\")\n",
    "            zero_std_cols.append(col)\n",
    "    X.drop(columns = zero_std_cols, inplace = True)\n",
    "\n",
    "    X_test = X_test[all_columns]\n",
    "\n",
    "    return (X, X_test, y, y_test, pgm_train_df, domain, target, attribute_dict, \n",
    "            features_to_encode, encoded_features, original_ranges, all_columns, zero_std_cols)\n",
    "\n",
    "\n",
    "def splitdf(df, target, train_ratio):\n",
    "    \n",
    "    n = len(df)\n",
    "\n",
    "    idxs = np.array(range(n))\n",
    "    np.random.shuffle(idxs)\n",
    "\n",
    "    train_rows, test_rows = idxs[:int(train_ratio * n)], idxs[int(train_ratio * n):]\n",
    "    df_train, df_test = df.iloc[train_rows, :], df.iloc[test_rows, :]\n",
    "\n",
    "    df_X_train, df_y_train = df_train.loc[:, df_train.columns != target], df_train.loc[:, df_train.columns == target]\n",
    "    df_X_test, df_y_test = df_test.loc[:, df_test.columns != target], df_test.loc[:, df_test.columns == target]\n",
    "\n",
    "    return (df_X_train, df_y_train, df_X_test, df_y_test)\n",
    "\n",
    "\n",
    "def get_features_to_encode(dataset):\n",
    "    if dataset == \"adult\":\n",
    "        features_to_encode = ['workclass', 'education', 'marital-status', 'occupation', 'relationship', 'race', 'native-country']\n",
    "    elif dataset == \"ACSincome\" or \"ACSincome-LIN\":\n",
    "        features_to_encode = ['COW', 'MAR', 'RELP', 'RAC1P']\n",
    "    elif dataset == \"ACSemployment\":\n",
    "        features_to_encode = ['MAR', 'RELP', 'CIT', 'MIL', 'ANC', 'RAC1P']\n",
    "    elif dataset == \"ACSmobility\":\n",
    "        features_to_encode = ['MAR', 'CIT', 'MIL', 'ANC', 'RELP', 'RAC1P', 'GCL', 'COW', 'ESR']\n",
    "    elif dataset == \"ACSPublicCoverage\":\n",
    "        features_to_encode = ['MAR', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'ESR', 'FER', 'RAC1P']\n",
    "    elif dataset == \"ACSTravelTime\":\n",
    "        features_to_encode = ['MAR', 'ESP', 'MIG', 'RELP', 'RAC1P', 'CIT', 'OCCP', 'JWTR']\n",
    "    elif dataset == \"titanic\":\n",
    "        features_to_encode = ['Pclass', 'Cabin', 'Embarked']\n",
    "    return features_to_encode\n",
    "\n",
    "\n",
    "def add_and_subsets_cols_to_test_set(X_test, columns):\n",
    "    for col_val in columns:\n",
    "        if col_val not in X_test.columns:\n",
    "            X_test[col_val] = 0\n",
    "    return X_test[columns]\n",
    "\n",
    "\n",
    "def selectTargetMarginals(cols, target, mode='target-pairs'):\n",
    "    out = []\n",
    "    if mode == 'target-pairs':\n",
    "        for col in cols:\n",
    "            if col != target:\n",
    "                out.append((col, target))\n",
    "    elif mode == 'target-triplets':\n",
    "        cols_new = list(cols)\n",
    "        cols_new.remove(target)\n",
    "        tmp_pairs = combinations(cols_new, 2)\n",
    "        out = [(t[0], t[1], target) for t in tmp_pairs]\n",
    "    elif mode == 'target-pairs-target-triplets':\n",
    "        out = []\n",
    "        for col in cols:\n",
    "            if col != target:\n",
    "                out.append((col, target))\n",
    "        cols_new = list(cols)\n",
    "        cols_new.remove(target)\n",
    "        tmp_pairs = combinations(cols_new, 2)\n",
    "        out.extend([(t[0], t[1], target) for t in tmp_pairs])\n",
    "    elif mode == \"all-pairs\":\n",
    "        out = list(combinations(cols, 2))\n",
    "    elif mode == 'all-triplets':\n",
    "        out = list(combinations(cols, 3))\n",
    "    elif mode == \"no-target-pairs\":\n",
    "        cols_new = list(cols)\n",
    "        cols_new.remove(target)\n",
    "        out = combinations(cols_new, 2)\n",
    "    return out\n",
    "\n",
    "def get_bound_XTX(attribute_dict, target, features_to_encode, one_hot):\n",
    "\n",
    "    if not one_hot: # then data is binary synthetic data\n",
    "        bound_X = np.sqrt(np.sum([max(attribute_dict[f])**2 for f in attribute_dict if f!=target]))\n",
    "        bound_XTX = bound_X**2\n",
    "        \n",
    "    elif one_hot: # follow sensitivity computation as described in paper\n",
    "        bound_XTX = len(attribute_dict.keys()) - 1    #excludes target\n",
    "        bound_X = np.sqrt(bound_XTX)\n",
    "\n",
    "    return bound_XTX, bound_X\n",
    "\n",
    "def normalize_minus1_1(X, attribute_dict, encoded_features):\n",
    "    original_ranges = {attr: [min(attribute_dict[attr]), max(attribute_dict[attr])]\n",
    "                       for attr in attribute_dict.keys()}\n",
    "\n",
    "    X_out = pd.DataFrame()\n",
    "\n",
    "    for col in X.columns:\n",
    "\n",
    "        # this is in case the test set has columns of zeros\n",
    "        if len(set(X[col])) == 1:\n",
    "            X_out[col] = X[col]\n",
    "\n",
    "        # if the column corresponds to a categorical feature that has been one-hot encoded, keep it in domain [0, 1]\n",
    "        if col in encoded_features:\n",
    "            X_out[col] = X[col]\n",
    "\n",
    "        # for all other features, rescale to [-1, 1]\n",
    "        else:\n",
    "            colmin = original_ranges[col][0]\n",
    "            colmax = original_ranges[col][1]\n",
    "\n",
    "            col_1s = (1 - (-1)) * ((X[col] - colmin) / (colmax - colmin)) - 1\n",
    "            X_out[col] = col_1s\n",
    "\n",
    "    return X_out\n",
    "\n",
    "\n",
    "# AIM Utils ############################################################################################################\n",
    "\n",
    "class PGMsynthesizer():\n",
    "\n",
    "    # this class is a wrapper for constructing the AIM model\n",
    "\n",
    "    def __init__(self, data, epsilon, delta, measurements, model_size, max_iters, num_synth_rows):\n",
    "        self.epsilon = epsilon\n",
    "        self.delta = delta\n",
    "        self.data = data\n",
    "        self.measurements = measurements\n",
    "        self.model_size = model_size\n",
    "        self.max_iters = max_iters\n",
    "        self.num_synth_rows = num_synth_rows\n",
    "        self.aim_model = None\n",
    "        self.synth = None\n",
    "        self.G = None\n",
    "        self.ans_wkld = None\n",
    "\n",
    "    def mstsynthesizer(self):\n",
    "        self.synth, self.G = mst.mst(self.data, self.epsilon, self.delta)\n",
    "\n",
    "    def mwemsynthesizer(self):\n",
    "        self.synth, self.G = mwem.mwem_pgm(self.data, self.epsilon, workload=self.measurements)\n",
    "\n",
    "    def v13synthesizer(self):\n",
    "        v13model = v13.V13(self.epsilon, self.delta)\n",
    "        self.synth, self.G = v13model.run(self.data, self.measurements)\n",
    "\n",
    "    def aimsynthesizer(self):\n",
    "        aim_model = aim.AIM(epsilon=self.epsilon, delta=self.delta, max_iters=self.max_iters,\n",
    "                           max_model_size=self.model_size)\n",
    "        self.synth, self.G, self.ans_wkld = aim_model.run(self.data, self.measurements, self.num_synth_rows,\n",
    "                                                         output_graphical_model=True)\n",
    "\n",
    "\n",
    "# DQ Query Approximation Utils #########################################################################################\n",
    "\n",
    "def expand_W(W, attribute_dict):\n",
    "    # add symmetric entries\n",
    "    W_expanded = copy.deepcopy(W)\n",
    "    for el in W:\n",
    "        W_expanded[el[1], el[0]] = W[el].T\n",
    "\n",
    "    # add (x, x) pairs\n",
    "    for col in attribute_dict.keys():\n",
    "        table_with_col = [W_expanded[tple] for tple in W_expanded if tple[0] == col][0]\n",
    "        col_counts = np.sum(table_with_col, axis=1)\n",
    "        W_expanded[col, col] = np.diag(col_counts)\n",
    "\n",
    "    return W_expanded\n",
    "\n",
    "class Chebyshev:\n",
    "    \"\"\"\n",
    "    Chebyshev(a, b, n, func)\n",
    "    Given a function func, lower and upper limits of the interval [a,b],\n",
    "    and maximum degree n, this class computes a Chebyshev approximation\n",
    "    of the function.\n",
    "    Method eval(x) yields the approximated function value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, a, b, n, func):\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "        self.func = func\n",
    "\n",
    "        bma = 0.5 * (b - a)\n",
    "        bpa = 0.5 * (b + a)\n",
    "        f = [func(math.cos(math.pi * (k + 0.5) / n) * bma + bpa) for k in range(n)]\n",
    "        fac = 2.0 / n\n",
    "        self.c = [fac * sum([f[k] * math.cos(math.pi * j * (k + 0.5) / n)\n",
    "                             for k in range(n)]) for j in range(n)]\n",
    "\n",
    "    def eval(self, x):\n",
    "        a, b = self.a, self.b\n",
    "        y = (2.0 * x - a - b) * (1.0 / (b - a))\n",
    "        y2 = 2.0 * y\n",
    "        (d, dd) = (self.c[-1], 0)  # Special case first step for efficiency\n",
    "        for cj in self.c[-2:0:-1]:  # Clenshaw's recurrence\n",
    "            (d, dd) = (y2 * d - dd + cj, d)\n",
    "        return y * d - dd + 0.5 * self.c[0]  # Last step is different\n",
    "\n",
    "\n",
    "def phi_logit(x):\n",
    "    return -math.log(1 + math.exp(-x))\n",
    "\n",
    "def logit_2(x):\n",
    "    return math.log(1 + math.exp(x))\n",
    "\n",
    "\n",
    "def get_ZTZ(W, attribute_dict, columns, features_to_encode, rescale):\n",
    "    \"\"\"\n",
    "        W: [dict] marginal tables in the form {(\"feature_A\", \"feature_B\"); m_A x m_B np.array of counts, ...}\n",
    "        attribute_dict: [dict] attribute levels, {\"attr_A\": list of ordered possible levels for attr_A, ...}\n",
    "                                - should include target\n",
    "        columns: [list] list of names of post-encoding attributes\n",
    "        features_to_encode: [list] list of features requiring 1-hot encoding}\n",
    "        rescale: [boolean] True if rescaling numerical non binary attributes in [-1, 1], TBC\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize ZTZ as a DataFrame with *named* columns and rows\n",
    "    base_matrix = np.zeros((len(columns), len(columns)))\n",
    "    ZTZ = pd.DataFrame(base_matrix, columns=columns, index=columns)\n",
    "\n",
    "    # loop through attribute pairs\n",
    "\n",
    "    for a, attr_a in enumerate(columns):\n",
    "        for b, attr_b in enumerate(columns[a:]):\n",
    "\n",
    "            # root name of the attributes\n",
    "            attr_a_orig = attr_a.split(\"_\")[0]\n",
    "            attr_b_orig = attr_b.split(\"_\")[0]\n",
    "            # possible level values\n",
    "            a_values = attribute_dict[attr_a_orig]\n",
    "            b_values = attribute_dict[attr_b_orig]\n",
    "            if rescale:\n",
    "                if attr_a_orig not in features_to_encode:\n",
    "                    a_range_min, a_range_max = min(a_values), max(a_values)\n",
    "                    a_values = [(1 -(-1)) * ((val - a_range_min) / (a_range_max - a_range_min)) - 1 for val in a_values]\n",
    "                if attr_b_orig not in features_to_encode:\n",
    "                    b_range_min, b_range_max = min(b_values), max(b_values)\n",
    "                    b_values = [(1 -(-1)) * ((val - b_range_min) / (b_range_max - b_range_min)) - 1 for val in b_values]\n",
    "\n",
    "            # case 1: a and b are both ordinal\n",
    "            if attr_a_orig not in features_to_encode and attr_b_orig not in features_to_encode:\n",
    "                mu_ab = W[(attr_a_orig, attr_b_orig)]\n",
    "                for j, j_value in enumerate(a_values):\n",
    "                    for k, k_value in enumerate(b_values):\n",
    "                        ZTZ[attr_a][attr_b] += j_value * k_value * mu_ab[j, k]\n",
    "\n",
    "            # case 2.1: a is ordinal, b is encoded\n",
    "            elif attr_a_orig not in features_to_encode and attr_b_orig in features_to_encode:\n",
    "                mu_ab = W[(attr_a_orig, attr_b_orig)]\n",
    "                t = int(float(attr_b.split(\"_\")[-1]))  # get level number ***** ASSUMES LEVELS CORRESPOND TO THE NAMES ******\n",
    "                ZTZ[attr_a][attr_b] = np.sum(np.multiply(mu_ab[:, t], a_values))\n",
    "\n",
    "            # case 2.2: a is encoded, b is ordinal\n",
    "            elif attr_a_orig in features_to_encode and attr_b_orig not in features_to_encode:\n",
    "                mu_ab = W[(attr_a_orig, attr_b_orig)]\n",
    "                s = int(float(attr_a.split(\"_\")[-1]))  # get level number ***** ASSUMES LEVELS CORRESPOND TO THE NAMES ******\n",
    "                ZTZ[attr_a][attr_b] = np.sum(np.multiply(mu_ab[s, :], b_values))\n",
    "\n",
    "            # case 3: a and b are both encoded\n",
    "            elif attr_a_orig in features_to_encode and attr_b_orig in features_to_encode:\n",
    "                s = int(float(attr_a.split(\"_\")[-1]))  # get level number ***** ASSUMES LEVELS CORRESPOND TO THE NAMES ******\n",
    "                t = int(float(attr_b.split(\"_\")[-1])) # get level number ***** ASSUMES LEVELS CORRESPOND TO THE NAMES ******\n",
    "                mu_ab = W[(attr_a_orig, attr_b_orig)]\n",
    "                ZTZ[attr_a][attr_b] = mu_ab[s, t]\n",
    "\n",
    "    # copy lower tri to upper tri\n",
    "    ZTZ = ZTZ + ZTZ.T - np.diag(np.diag(ZTZ))\n",
    "\n",
    "    return ZTZ\n",
    "\n",
    "def one_hot_encode(df, features_to_encode, attribute_dict):\n",
    "\n",
    "    encoded_df = df.copy()\n",
    "\n",
    "    for feature in features_to_encode:\n",
    "        unique_levels = attribute_dict[feature]\n",
    "        for level in unique_levels:\n",
    "            encoded_df[f'{feature}_{level}'] = (df[feature] == level).astype(int)\n",
    "            \n",
    "    encoded_df.drop(features_to_encode, axis=1, inplace=True)\n",
    "\n",
    "    # drop first\n",
    "    for col in encoded_df.columns:\n",
    "        if col.endswith(\"_0\"):\n",
    "            encoded_df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "    encoded_df = encoded_df.copy()\n",
    "\n",
    "    return encoded_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c51137f-13d1-4b1a-8af3-29271f931207",
   "metadata": {},
   "source": [
    "REGRESSION UTILS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bc49a94-4f9b-40f9-9b5d-fe1c2f15ea0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegressionObjective():\n",
    "    @staticmethod\n",
    "    def sigmoid_v2(x, theta):\n",
    "        z = np.dot(x, theta)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    @staticmethod\n",
    "    def hypothesis(theta, x):\n",
    "        return LogisticRegressionObjective.sigmoid_v2(x, theta)\n",
    "\n",
    "    @staticmethod\n",
    "    def loss(theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        h = LogisticRegressionObjective.hypothesis(theta, x)\n",
    "        return -(1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "\n",
    "    @staticmethod\n",
    "    def gradient(theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        h = LogisticRegressionObjective.hypothesis(theta, x)\n",
    "        return (1 / m) * np.dot(x.T, (h - y))\n",
    "\n",
    "def genobjpert_get_params(X, epsilon, delta, lmda, zeta):\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    delta_thrs = 2*lmda/epsilon\n",
    "    Delta = delta_thrs\n",
    "    b_var = zeta**2 * (8 * np.log(2/delta) + 4*epsilon) / (epsilon**2) * np.eye(d)\n",
    "    b = np.random.multivariate_normal(np.zeros((b_var.shape[0], )), b_var)\n",
    "\n",
    "    return Delta, b\n",
    "\n",
    "def dp_objective(theta, X, y, n, d, Delta, b):\n",
    "\n",
    "    base_loss = LogisticRegressionObjective.loss(theta, X, y)\n",
    "    regularizer = 1/n * 0   #assumed zero\n",
    "    sec_term = Delta/(2*n) * np.dot(theta.T, theta)\n",
    "    third_term = np.dot(b.T, theta)/n\n",
    "\n",
    "    return base_loss + regularizer + sec_term + third_term\n",
    "\n",
    "def dp_gradient(theta, X, y, n, d, Delta, b):\n",
    "    base_gradient = LogisticRegressionObjective.gradient(theta, X, y)\n",
    "\n",
    "    reg_term = 1/n * np.zeros((d,))  # Assumed zero\n",
    "    second_term = Delta/n * theta\n",
    "    third_term = b/n\n",
    "\n",
    "    return base_gradient + reg_term + second_term + third_term\n",
    "\n",
    "def get_dp_approx_ll(theta, yTX, XTXy2, a, b, c, n):\n",
    "    dp_approx_ll = n * a + np.dot(theta, yTX) * b + np.dot(np.dot(theta, XTXy2), theta) * c\n",
    "    return dp_approx_ll\n",
    "\n",
    "class SSApproxLL():\n",
    "\n",
    "    def __init__(self, ch, yTX, XTXy2, n, penalty, alpha):\n",
    "        self.n = n\n",
    "        self.ch = ch\n",
    "        self.penalty = penalty\n",
    "        self.alpha = alpha\n",
    "        self.theta = None\n",
    "        self.yTX = yTX\n",
    "        self.XTXy2 = XTXy2\n",
    "\n",
    "    def fit(self):\n",
    "        self.optimize()\n",
    "        return self\n",
    "\n",
    "    def log_likelihood(self, theta):\n",
    "        a, b, c = self.ch.c\n",
    "        term = get_dp_approx_ll(theta, self.yTX, self.XTXy2, a, b, c, self.n)\n",
    "        term = 1 / self.n * term\n",
    "        return term\n",
    "\n",
    "    def optimize(self):\n",
    "\n",
    "        def l2_penalty(theta):\n",
    "            return np.sum(theta ** 2)\n",
    "\n",
    "        x0 = [.0] * len(self.yTX)\n",
    "\n",
    "        if self.penalty == None:\n",
    "            res = minimize(lambda theta: -self.log_likelihood(theta),\n",
    "                           x0,\n",
    "                           method='L-BFGS-B',\n",
    "                           options={'maxiter': 10000},\n",
    "                           tol=0.00001)\n",
    "            theta_star = res.x\n",
    "            fun = res.fun\n",
    "\n",
    "        elif self.penalty == 'l2':\n",
    "            res = minimize(lambda theta: -self.log_likelihood(theta) + self.alpha * l2_penalty(theta),\n",
    "                           x0,\n",
    "                           method='L-BFGS-B',\n",
    "                           options={'maxiter': 10000},\n",
    "                           tol=0.00001)\n",
    "            theta_star = res.x\n",
    "            fun = res.fun - self.alpha * l2_penalty(theta_star)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('Unknown penalty type, choose None or l2')\n",
    "\n",
    "        self.theta = theta_star\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = np.dot(X, self.theta.T)\n",
    "        y_pred_proba = expit(z)\n",
    "        return (y_pred_proba)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        z = np.dot(X, self.theta.T)\n",
    "        y_pred_proba = expit(z)\n",
    "        y_pred = 2 * ((y_pred_proba >= threshold).astype(int)) - 1\n",
    "        print(\"y pred dpapproxss\", y_pred)\n",
    "        return (y_pred)\n",
    "\n",
    "def get_aim_model(pgm_train_df, domain, target, marginals_pgm, epsilon, delta, model_size, max_iters, n_samples):\n",
    "    pgm_dataset = Dataset(pgm_train_df, domain)\n",
    "    mrgs = selectTargetMarginals(pgm_train_df.columns, target, mode=marginals_pgm)\n",
    "    mrgs_wkld = Workload((mrg, sparse.identity) for mrg in mrgs)\n",
    "    pgm = PGMsynthesizer(pgm_dataset, epsilon, delta, mrgs_wkld, model_size, max_iters, n_samples)\n",
    "    pgm.aimsynthesizer()\n",
    "    return pgm, mrgs\n",
    "\n",
    "def testLogReg(theta, X_test, y_test):\n",
    "    logits = np.dot(X_test, theta)\n",
    "    probabilities = 1 / (1 + np.exp(-logits))\n",
    "    auc = roc_auc_score(y_test, probabilities)\n",
    "    return auc\n",
    "\n",
    "def testLinReg(theta, X_test, y_test):\n",
    "    y_pred = np.dot(X_test, theta)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b768b927-f229-4d48-ad2f-87a2eb73091a",
   "metadata": {},
   "source": [
    "REGRESSION METHODS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1420179-8642-4d0a-bf55-a7990d151f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def public_logreg(X, y):\n",
    "    model = LogisticRegression(penalty=None, fit_intercept=False, max_iter=2000)\n",
    "    model.fit(X.to_numpy(), y.to_numpy().ravel())\n",
    "    theta = model.coef_.ravel()   # probabilities for class 1\n",
    "    theta = pd.DataFrame(theta)\n",
    "    print(X.shape)\n",
    "    print(theta.shape)\n",
    "    theta.set_index(X.columns, inplace=True)\n",
    "    return theta\n",
    "\n",
    "def dp_query_approx_ss_logreg(ZTZ, all_columns, target, n, cheb, phi_logit, C=1.0):\n",
    "\n",
    "    XTXy2 = ZTZ.loc[all_columns, all_columns]\n",
    "    XTy = ZTZ.loc[all_columns, target]\n",
    "    alpha = 1 / (n * C)\n",
    "    model = SSApproxLL(cheb, XTy, XTXy2, n, penalty=None, alpha=alpha)\n",
    "    model.fit()\n",
    "    theta = model.theta\n",
    "    theta = pd.DataFrame(theta)\n",
    "    theta.set_index(all_columns, inplace=True)\n",
    "\n",
    "    return theta\n",
    "\n",
    "def objective_perturbation_method(X, y, epsilon, delta, bound_X, bound_y):\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    max_row_norm = bound_X\n",
    "    zeta = max_row_norm\n",
    "    lmda = smooth_const = max_row_norm ** 2 / 4\n",
    "\n",
    "    def sigmoid_v2(x, theta):\n",
    "        z = np.dot(x, theta)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def hypothesis(theta, x):\n",
    "        return sigmoid_v2(x, theta)\n",
    "\n",
    "    def cost_function(theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        h = hypothesis(theta, x)\n",
    "        return -(1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
    "\n",
    "    def gradient_fun(theta, x, y):\n",
    "        m = x.shape[0]\n",
    "        h = hypothesis(theta, x)\n",
    "        return (1 / m) * np.dot(x.T, (h - y))\n",
    "\n",
    "    # Run the object perturbation\n",
    "    Delta, b = genobjpert_get_params(X.to_numpy(), epsilon, delta, lmda, zeta)\n",
    "\n",
    "    # Run the iteration\n",
    "    # fmin_tnc returns the convergence message as third argument in its output\n",
    "    # 0 means local minimium reached, 1 and 2 means convergence by function value or theta value\n",
    "    # 3 is maximum number of iterations reached, 4 linear search failed\n",
    "    finish_opt = False\n",
    "    patience = 3\n",
    "\n",
    "    while not finish_opt and patience > 0:\n",
    "\n",
    "        theta0 = np.random.normal(loc=0, scale=0.01, size=X.shape[1]).reshape(-1, )\n",
    "        theta_opt = fmin_tnc(func=dp_objective, x0=theta0, fprime=dp_gradient, maxfun=10000, disp=0,\n",
    "                             args=(X.to_numpy(), y.to_numpy().reshape(-1, ), n, d, Delta, b))\n",
    "\n",
    "        theta_final, n_it_run, final_message = theta_opt\n",
    "\n",
    "        if final_message in [0, 1, 2]:\n",
    "            finish_opt = True\n",
    "        else:\n",
    "            patience -= 1\n",
    "\n",
    "    theta_final = pd.DataFrame(theta_final)\n",
    "    theta_final.set_index(X.columns, inplace=True)\n",
    "\n",
    "    return theta_final\n",
    "\n",
    "def public_linreg(X, y):\n",
    "\n",
    "    # X_vec, y_vec = X.values, y.values.ravel()\n",
    "    # XTX = np.dot(X_vec.T, X_vec)\n",
    "    # XTy = np.dot(X_vec.T, y_vec)\n",
    "    # theta = np.linalg.solve(XTX, XTy)\n",
    "\n",
    "    regr = LinearRegression(fit_intercept=False)\n",
    "    regr.fit(X.values, y.values.ravel())\n",
    "    theta = regr.coef_\n",
    "    theta = pd.DataFrame(theta)\n",
    "    theta = theta.set_index(X.columns)\n",
    "\n",
    "    return theta\n",
    "\n",
    "def AdaSSP_linear_regression(X, y, epsilon, delta, rho, bound_X, bound_y, bound_XTX, all_columns):\n",
    "    \"\"\"Returns DP linear regression model and metrics using AdaSSP. AdaSSP is described in Algorithm 2 of\n",
    "        https://arxiv.org/pdf/1803.02596.pdf.\n",
    "\n",
    "    Args:\n",
    "        X: df feature vectors\n",
    "        y: df of labels\n",
    "        epsilon: model needs to meet (epsilon, delta)-DP.\n",
    "        delta: model needs to meet (epsilon, delta)-DP.\n",
    "        rho: failure probability, default of 0.05 is from original paper\n",
    "        bound_X, bound y: bounds on the L2 sensitivity\n",
    "        bound_XTX: bound on the sensitivity of XTX (is data is one hot encoded, XTX is sparser, sensitivity must be adapted)\n",
    "        X_test, y_test: test data for evaluation\n",
    "\n",
    "    Returns:\n",
    "        theta_dp: regression coefficients\n",
    "        mse_dp: mean squared error\n",
    "        r2_dp: r2 score\n",
    "    \"\"\"\n",
    "\n",
    "    n, d = X.shape\n",
    "\n",
    "    XTX = np.dot(X.T, X)\n",
    "    XTy = np.dot(X.T, y).flatten()\n",
    "\n",
    "    eigen_min = max(0, np.amin(np.linalg.eigvals(XTX)))\n",
    "    z = np.random.normal(0, 1, size=1)\n",
    "    sensitivity = np.sqrt(np.log(6 / delta)) / (epsilon / 3)\n",
    "    eigen_min_dp = max(0,\n",
    "                       eigen_min + sensitivity * (bound_XTX) * z -\n",
    "                       (bound_XTX) * np.log(6 / delta) / (epsilon / 3))\n",
    "    lambda_dp = max(0,\n",
    "                    np.sqrt(d * np.log(6 / delta) * np.log(2 * (d ** 2) / rho)) * (bound_XTX) /\n",
    "                    (epsilon / 3) - eigen_min_dp)\n",
    "\n",
    "    tri = np.triu(np.random.normal(0, 1, (d, d)))\n",
    "    Zsym = tri + tri.T - np.diag(np.diag(tri))\n",
    "    XTX_dp = XTX + sensitivity * (bound_XTX) * Zsym\n",
    "    print(\"epsilon =\", epsilon, \"XTX_dp noise variance\", sensitivity * (bound_XTX))\n",
    "\n",
    "    z = np.random.normal(0, 1, size=(d,))\n",
    "    XTy_dp = XTy + sensitivity * bound_X * bound_y * z\n",
    "    XTX_dp_reg = XTX_dp + lambda_dp * np.eye(d)\n",
    "\n",
    "    theta_dp = np.linalg.solve(XTX_dp_reg, XTy_dp)\n",
    "\n",
    "    theta_dp = pd.DataFrame(theta_dp)\n",
    "    theta_dp = theta_dp.set_index(X.columns)\n",
    "\n",
    "    return theta_dp\n",
    "\n",
    "def dp_query_ss_linreg(ZTZ, all_columns, target):\n",
    "\n",
    "    XTX = ZTZ.loc[all_columns, all_columns]\n",
    "    XTy = ZTZ.loc[all_columns, target]\n",
    "\n",
    "    # get estimator\n",
    "    theta_query_ss = np.linalg.solve(XTX, XTy)   \n",
    "    theta_query_ss = pd.DataFrame(theta_query_ss)   \n",
    "    theta_query_ss = theta_query_ss.set_index(all_columns)\n",
    "\n",
    "    return theta_query_ss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1787af8c-d508-4395-bab8-2b0d4e825980",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "89b14324-db8f-404e-bf80-02ac8799f1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AGEP': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94], 'COW': [0, 1, 2, 3, 4, 5, 6, 7], 'SCHL': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], 'MAR': [0, 1, 2, 3, 4], 'OCCP': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9], 'RELP': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], 'WKHP': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98], 'SEX': [0, 1], 'RAC1P': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'PINCP': [0, 1]}\n",
      "one-hot encoding ACSincome...\n",
      "feature RELP_14 is a zero vector! Dropping it from training dataset\n",
      "feature RAC1P_3 is a zero vector! Dropping it from training dataset\n",
      "X.shape (14000, 39)\n",
      "X_test.shape (6000, 41)\n",
      "y.shape (14000, 1)\n",
      "y_test.shape (6000, 1)\n",
      "X bound 3.0\n",
      "y bound 1\n"
     ]
    }
   ],
   "source": [
    "dataset = 'ACSincome'\n",
    "csv_path = '../hd-datasets-master/clean/' + dataset + '.csv'\n",
    "meta_path = '../hd-datasets-master/clean/' + dataset + '-domain.json'\n",
    "epsilon = 0.05\n",
    "delta = 1e-5\n",
    "n_limit = 20_000\n",
    "train_ratio = 0.7\n",
    "one_hot = True\n",
    "\n",
    "# AIM model parameters\n",
    "model_size = 100  \n",
    "max_iters = 1000  \n",
    "PGMmarginals = 'all-pairs'\n",
    "\n",
    "target_dict = {'adult': 'income>50K', 'titanic': 'Survived', 'diabetes': 'Outcome',\n",
    "               'ACSemployment': 'ESR', 'ACSincome': 'PINCP', \"ACSmobility\": 'MIG',\n",
    "               \"ACSPublicCoverage\": 'PUBCOV', 'ACSTravelTime': 'JWMNP', 'logregbinary10': 'predicted',\n",
    "               'logregbinary5lowcorr': 'predicted', 'logregbinary7lowcorr': 'predicted',\n",
    "               'logregbinary10lowcorr': 'predicted', 'logregbinary20lowcorr': 'predicted',\n",
    "               'logregbinary30lowcorr': 'predicted'}\n",
    "\n",
    "np.random.seed(237)\n",
    "\n",
    "(X, X_test, y, y_test, \n",
    " pgm_train_df, domain, target, \n",
    " attribute_dict, features_to_encode, encoded_features, \n",
    " original_ranges, all_columns, zero_std_cols) = preprocess_data(dataset, target_dict, n_limit, train_ratio, one_hot)\n",
    "\n",
    "n, d = X.shape\n",
    "\n",
    "print(f\"X.shape {X.shape}\")\n",
    "print(f\"X_test.shape {X_test.shape}\")\n",
    "print(f\"y.shape {y.shape}\")\n",
    "print(f\"y_test.shape {y_test.shape}\")\n",
    "\n",
    "# bounds\n",
    "bound_XTX, bound_X = get_bound_XTX(attribute_dict, target, features_to_encode, one_hot)\n",
    "bound_y = 1 if one_hot else np.abs(np.max(attribute_dict[target]))\n",
    "\n",
    "print(f\"X bound {bound_X}\")\n",
    "print(f\"y bound {bound_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0e3cf-1bb9-4767-9026-6c0853b5928e",
   "metadata": {},
   "source": [
    "PUBLIC LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "eebe4e46-8625-49dd-b6c3-917cadc00a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14000, 39)\n",
      "(39, 1)\n",
      "(41, 1)\n"
     ]
    }
   ],
   "source": [
    "theta_public = public_logreg(X, y)\n",
    "theta_public = theta_public.reindex(index=all_columns, fill_value=0)\n",
    "print(theta_public.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74681638-cc85-4a21-b2d4-3931774598ba",
   "metadata": {},
   "source": [
    "TRAIN AND SAVE AIM MODEL, OBTAIN MARGINAL TABLES AND SYNTHETIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "9ccd9058-5188-4140-8e82-3353b543fd39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Sigma 856.9186237601189\n",
      "(!!!!!!!!!!!!!!!!!!!!!!) Reducing sigma 428.45931188005943\n",
      "(!!!!!!!!!!!!!!!!!!!!!!) Reducing sigma 214.22965594002972\n",
      "(!!!!!!!!!!!!!!!!!!!!!!) Reducing sigma 107.11482797001486\n",
      "Generating Data...\n"
     ]
    }
   ],
   "source": [
    "# 1) get AIM model and save it\n",
    "aim_model, workload = get_aim_model(pgm_train_df, domain, target, PGMmarginals, epsilon, delta, model_size, max_iters, len(X))\n",
    "aim_model_graph = aim_model.G\n",
    "with open('aim_model.pkl', 'wb') as f:\n",
    "    dill.dump(aim_model_graph, f)\n",
    "\n",
    "# 2) load AIM model and get marginal tables and synthetic data X_synth, y_synth\n",
    "with open('aim_model.pkl', 'rb') as f:\n",
    "    aim_model_graph = dill.load(f)\n",
    "\n",
    "aim_ans_wkld = {cl: aim_model_graph.project(cl) for cl in workload}\n",
    "W = {key: aim_ans_wkld[key].__dict__['values'] for key in aim_ans_wkld}\n",
    "\n",
    "synth = aim_model_graph.synthetic_data(rows=n).df\n",
    "synth_X, synth_y = synth.loc[:, synth.columns != target], synth.loc[:, synth.columns == target]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918fcc99-6994-4cca-ac17-bc1db84da5c0",
   "metadata": {},
   "source": [
    "DPQUERYSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3cb9f443-4f5b-43b3-8f8e-a93615fb1355",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['RELP_16', 'WKHP', 'MAR_4', 'RELP_4', 'RELP_15', 'COW_7', 'RELP_14',\n",
      "       'RAC1P_5', 'OCCP', 'AGEP', 'SEX', 'COW_1', 'RAC1P_2', 'MAR_2',\n",
      "       'RAC1P_1', 'RELP_5', 'COW_3', 'RELP_13', 'RELP_12', 'RELP_9', 'RELP_6',\n",
      "       'RELP_8', 'RELP_3', 'RELP_11', 'MAR_3', 'RELP_10', 'RAC1P_3', 'COW_6',\n",
      "       'RELP_7', 'RELP_17', 'RELP_1', 'RAC1P_4', 'RELP_2', 'COW_4', 'COW_2',\n",
      "       'RAC1P_8', 'RAC1P_7', 'RAC1P_6', 'COW_5', 'SCHL', 'MAR_1'],\n",
      "      dtype='object')\n",
      "Index(['RELP_16', 'WKHP', 'MAR_4', 'RELP_4', 'RELP_15', 'COW_7', 'RELP_14',\n",
      "       'RAC1P_5', 'OCCP', 'AGEP', 'SEX', 'COW_1', 'RAC1P_2', 'MAR_2',\n",
      "       'RAC1P_1', 'RELP_5', 'COW_3', 'RELP_13', 'RELP_12', 'RELP_9', 'RELP_6',\n",
      "       'RELP_8', 'RELP_3', 'RELP_11', 'MAR_3', 'RELP_10', 'RAC1P_3', 'COW_6',\n",
      "       'RELP_7', 'RELP_17', 'RELP_1', 'RAC1P_4', 'RELP_2', 'COW_4', 'COW_2',\n",
      "       'RAC1P_8', 'RAC1P_7', 'RAC1P_6', 'COW_5', 'SCHL', 'MAR_1', 'PINCP'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "W_expanded = expand_W(W, attribute_dict)\n",
    "\n",
    "# get Chebyshev\n",
    "r = 6\n",
    "degree = 3\n",
    "C = 1.0\n",
    "cheb = Chebyshev(-r, r, degree, phi_logit)\n",
    "\n",
    "# get ZTZ, can be computed once for both linear and logistic regression in final notebook\n",
    "print(all_columns)\n",
    "all_attributes_expanded = all_columns.append(pd.Index([target]))\n",
    "print(all_attributes_expanded)\n",
    "ZTZ = get_ZTZ(W_expanded, attribute_dict, all_attributes_expanded, features_to_encode, rescale=one_hot)\n",
    "\n",
    "# approximate sufficient statistics\n",
    "theta_dpqueryss = dp_query_approx_ss_logreg(ZTZ, all_columns, target, n, cheb, phi_logit, C=1.0)\n",
    "theta_dpqueryss = theta_dpqueryss.reindex(index=all_columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ee2b5b-1059-48b4-b4cc-bcd7a3a3d5a6",
   "metadata": {},
   "source": [
    "DP AIM SYNTHETIC DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f5c0c6aa-28da-4154-8472-c77c234c5abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature RELP_12 is a zero vector! Dropping in at train time, adding corresponding zeros in theta\n",
      "feature RAC1P_6 is a zero vector! Dropping in at train time, adding corresponding zeros in theta\n",
      "(14000, 39) (14000, 1)\n",
      "(14000, 39)\n",
      "(39, 1)\n",
      "(41, 1)\n"
     ]
    }
   ],
   "source": [
    "synth_X_ohe = one_hot_encode(synth_X, features_to_encode, attribute_dict)\n",
    "\n",
    "zero_std_cols = []\n",
    "for col in synth_X_ohe.columns:\n",
    "    if np.std(synth_X_ohe[col]) == 0:\n",
    "        print(f\"feature {col} is a zero vector! Dropping in at train time, adding corresponding zeros in theta\")\n",
    "        zero_std_cols.append(col)\n",
    "synth_X_ohe.drop(columns = zero_std_cols, inplace = True)\n",
    "\n",
    "print(synth_X_ohe.shape, synth_y.shape)\n",
    "theta_aimsynth = public_logreg(synth_X_ohe, synth_y).T\n",
    "for col in zero_std_cols:\n",
    "    theta_aimsynth[col] = 0\n",
    "\n",
    "theta_aimsynth = theta_aimsynth.reindex(columns=all_columns, fill_value=0)\n",
    "\n",
    "for i,col in enumerate(all_columns):\n",
    "    assert col == theta_aimsynth.columns[i] \n",
    "\n",
    "theta_aimsynth = theta_aimsynth.T.to_numpy()\n",
    "print(theta_aimsynth.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9934b3-5cd0-40f1-9817-8394920c9540",
   "metadata": {},
   "source": [
    "DP OBJECTIVE PERTURBATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "21d81259-8d76-4be0-84c2-02f91023ab39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_v/3q83tdz16dz82yk206z719v40000gn/T/ipykernel_37377/1562721203.py:5: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n",
      "/var/folders/_v/3q83tdz16dz82yk206z719v40000gn/T/ipykernel_37377/1562721203.py:15: RuntimeWarning: divide by zero encountered in log\n",
      "  return -(1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
      "/var/folders/_v/3q83tdz16dz82yk206z719v40000gn/T/ipykernel_37377/1562721203.py:15: RuntimeWarning: invalid value encountered in multiply\n",
      "  return -(1 / m) * np.sum(y * np.log(h) + (1 - y) * np.log(1 - h))\n",
      "/Users/ceciliaferrando/Documents/UMASS/RESEARCH/DPsynthesisML/dpsynthesisml-pyenv310/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86: RuntimeWarning: invalid value encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n"
     ]
    }
   ],
   "source": [
    "theta_objpert = objective_perturbation_method(X, y, epsilon, delta, bound_X, bound_y)\n",
    "theta_objpert = theta_objpert.reindex(index=all_columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e61e54-6c5b-434d-bb23-01298c46df87",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4f41152c-c62c-46ea-8848-9a997e696f29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 1)\n",
      "(6000, 41)\n",
      "public auc: 0.8906380224759534\n",
      "dpqueryss auc: 0.7600603958522478\n",
      "aimsynth auc: 0.7674780369318711\n",
      "objpert auc: 0.695816612000273\n"
     ]
    }
   ],
   "source": [
    "print(theta_public.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "auc_public = testLogReg(theta_public, X_test, y_test)\n",
    "auc_dpqueryss = testLogReg(theta_dpqueryss, X_test, y_test)\n",
    "auc_aimsynth = testLogReg(theta_aimsynth, X_test, y_test)\n",
    "auc_objpert = testLogReg(theta_objpert, X_test, y_test)\n",
    "\n",
    "print(f\"public auc: {auc_public}\")\n",
    "print(f\"dpqueryss auc: {auc_dpqueryss}\")\n",
    "print(f\"aimsynth auc: {auc_aimsynth}\")\n",
    "print(f\"objpert auc: {auc_objpert}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74349f8f-80d0-47f6-a517-f6a52b8b78cf",
   "metadata": {},
   "source": [
    "# LINEAR REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ebf81041-9f4b-4bf4-8faf-089293379737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AGEP': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94], 'COW': [0, 1, 2, 3, 4, 5, 6, 7], 'SCHL': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], 'MAR': [0, 1, 2, 3, 4], 'RELP': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17], 'WKHP': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98], 'SEX': [0, 1], 'RAC1P': [0, 1, 2, 3, 4, 5, 6, 7, 8], 'PINCP': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}\n",
      "one-hot encoding ACSincome-LIN...\n",
      "X.shape (35000, 40)\n",
      "X_test.shape (15000, 40)\n",
      "y.shape (35000, 1)\n",
      "y_test.shape (15000, 1)\n",
      "X bound 2.8284271247461903\n",
      "y bound 1\n"
     ]
    }
   ],
   "source": [
    "dataset = 'ACSincome-LIN'\n",
    "csv_path = '../hd-datasets-master/clean/' + dataset + '.csv'\n",
    "meta_path = '../hd-datasets-master/clean/' + dataset + '-domain.json'\n",
    "epsilon = 0.1\n",
    "delta = 1e-5\n",
    "rho = 0.05\n",
    "n_limit = 50_000\n",
    "train_ratio = 0.7\n",
    "one_hot = True\n",
    "\n",
    "# AIM model parameters\n",
    "model_size = 100  \n",
    "max_iters = 1000  \n",
    "PGMmarginals = 'all-pairs'\n",
    "\n",
    "target_dict = {'adult': 'education-num', 'ACSincome-LIN': 'PINCP',\n",
    "               'ACSPublicCoverage': 'AGEP', 'ACSmobility': 'AGEP', 'linregbinary': 'predicted',\n",
    "               'linregbinary10': 'predicted',\n",
    "               'linregbinary30': 'predicted'}\n",
    "\n",
    "np.random.seed(237)\n",
    "\n",
    "(X, X_test, y, y_test, \n",
    " pgm_train_df, domain, target, \n",
    " attribute_dict, features_to_encode, encoded_features, \n",
    " original_ranges, all_columns, zero_std_cols) = preprocess_data(dataset, target_dict, n_limit, train_ratio, one_hot)\n",
    "\n",
    "n, d = X.shape\n",
    "\n",
    "print(f\"X.shape {X.shape}\")\n",
    "print(f\"X_test.shape {X_test.shape}\")\n",
    "print(f\"y.shape {y.shape}\")\n",
    "print(f\"y_test.shape {y_test.shape}\")\n",
    "\n",
    "# bounds\n",
    "bound_XTX, bound_X = get_bound_XTX(attribute_dict, target, features_to_encode, one_hot)\n",
    "bound_y = 1 if one_hot else np.abs(np.max(attribute_dict[target]))\n",
    "\n",
    "print(f\"X bound {bound_X}\")\n",
    "print(f\"y bound {bound_y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73ff5359-9326-459b-bc64-8fd022864e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                0\n",
      "RELP_3  -0.364429\n",
      "RELP_11 -0.165448\n",
      "AGEP     0.369039\n",
      "WKHP     0.702117\n",
      "RELP_2  -0.292610\n",
      "MAR_1   -0.038840\n",
      "RELP_9  -0.237859\n",
      "RELP_10 -0.214750\n",
      "RELP_12 -0.149002\n",
      "RAC1P_3 -0.171927\n",
      "RELP_8  -0.171160\n",
      "COW_6    0.047274\n",
      "COW_2    0.032219\n",
      "MAR_2    0.007129\n",
      "RELP_7  -0.333257\n",
      "RELP_15 -0.220413\n",
      "MAR_4   -0.057513\n",
      "SCHL     0.568543\n",
      "RELP_16 -0.511350\n",
      "RELP_17 -0.492388\n",
      "RELP_6  -0.258763\n",
      "COW_3    0.053908\n",
      "COW_4    0.015024\n",
      "RAC1P_1 -0.068195\n",
      "RELP_13 -0.080754\n",
      "RAC1P_2 -0.103597\n",
      "RELP_14 -0.004663\n",
      "RELP_1  -0.021526\n",
      "RAC1P_7 -0.055949\n",
      "RAC1P_4 -0.040506\n",
      "COW_1    0.023874\n",
      "RELP_5  -0.225361\n",
      "RELP_4  -0.312930\n",
      "SEX     -0.070258\n",
      "COW_7   -0.167859\n",
      "RAC1P_5  0.038817\n",
      "MAR_3   -0.075198\n",
      "COW_5   -0.134187\n",
      "RAC1P_6  0.017300\n",
      "RAC1P_8 -0.015281\n"
     ]
    }
   ],
   "source": [
    "theta_public = public_linreg(X, y)\n",
    "theta_public = theta_public.reindex(index=all_columns, fill_value=0)\n",
    "print(theta_public)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc9d98fe-0b46-4e6c-ace4-dd44897f4bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon = 0.1 XTX_dp noise variance 875.4141032733144\n",
      "                0\n",
      "RELP_3  -0.012321\n",
      "RELP_11 -0.017053\n",
      "AGEP     0.080523\n",
      "WKHP     0.117554\n",
      "RELP_2  -0.045103\n",
      "MAR_1   -0.015051\n",
      "RELP_9  -0.021711\n",
      "RELP_10 -0.026660\n",
      "RELP_12  0.023494\n",
      "RAC1P_3  0.010409\n",
      "RELP_8   0.005097\n",
      "COW_6    0.024837\n",
      "COW_2    0.046780\n",
      "MAR_2    0.010586\n",
      "RELP_7  -0.015974\n",
      "RELP_15 -0.019457\n",
      "MAR_4   -0.148051\n",
      "SCHL     0.102912\n",
      "RELP_16 -0.038006\n",
      "RELP_17 -0.080095\n",
      "RELP_6  -0.007002\n",
      "COW_3    0.061932\n",
      "COW_4    0.026492\n",
      "RAC1P_1 -0.018161\n",
      "RELP_13 -0.001698\n",
      "RAC1P_2 -0.009794\n",
      "RELP_14  0.008182\n",
      "RELP_1   0.014480\n",
      "RAC1P_7 -0.018932\n",
      "RAC1P_4 -0.019676\n",
      "COW_1   -0.005475\n",
      "RELP_5  -0.032798\n",
      "RELP_4  -0.003006\n",
      "SEX     -0.042380\n",
      "COW_7   -0.028793\n",
      "RAC1P_5  0.041380\n",
      "MAR_3   -0.027872\n",
      "COW_5   -0.047619\n",
      "RAC1P_6  0.013832\n",
      "RAC1P_8  0.016989\n"
     ]
    }
   ],
   "source": [
    "theta_adassp = AdaSSP_linear_regression(X, y, epsilon, delta, rho, bound_X, bound_y, bound_XTX, all_columns)\n",
    "theta_adassp = theta_adassp.reindex(index=all_columns, fill_value=0)\n",
    "print(theta_adassp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f111fb86-621f-4daa-b134-3f3d47877dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Sigma 429.83738951447367\n",
      "(!!!!!!!!!!!!!!!!!!!!!!) Reducing sigma 214.91869475723684\n"
     ]
    }
   ],
   "source": [
    "# 1) get AIM model and save it\n",
    "aim_model, workload = get_aim_model(pgm_train_df, domain, target, PGMmarginals, epsilon, delta, model_size, max_iters, n)\n",
    "aim_model_graph = aim_model.G\n",
    "with open('aim_model.pkl', 'wb') as f:\n",
    "    dill.dump(aim_model_graph, f)\n",
    "\n",
    "# 2) load AIM model and get marginal tables and synthetic data X_synth, y_synth\n",
    "with open('aim_model.pkl', 'rb') as f:\n",
    "    aim_model_graph = dill.load(f)\n",
    "\n",
    "aim_ans_wkld = {cl: aim_model_graph.project(cl) for cl in workload}\n",
    "W = {key: aim_ans_wkld[key].__dict__['values'] for key in aim_ans_wkld}\n",
    "\n",
    "synth = aim_model_graph.synthetic_data(rows=n).df\n",
    "synth_X, synth_y = synth.loc[:, synth.columns != target], synth.loc[:, synth.columns == target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4fb9de-7329-4f87-9def-bf5134f72e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "W_expanded = expand_W(W, attribute_dict)\n",
    "\n",
    "# approximate sufficient statistics\n",
    "all_attributes_expanded = all_columns.append(pd.Index([target]))\n",
    "ZTZ = get_ZTZ(W_expanded, attribute_dict, all_attributes_expanded, features_to_encode, rescale=one_hot)\n",
    "theta_dpqueryss = dp_query_ss_linreg(ZTZ, all_columns, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a53172-8930-4269-bfff-392efbde4434",
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_X_ohe = one_hot_encode(synth_X, features_to_encode, attribute_dict)\n",
    "synth_X_ohe = normalize_minus1_1(synth_X_ohe, attribute_dict, encoded_features)\n",
    "synth_y = normalize_minus1_1(synth_y, attribute_dict, encoded_features)\n",
    "\n",
    "zero_std_cols = []\n",
    "for col in synth_X_ohe.columns:\n",
    "    if np.std(synth_X_ohe[col]) == 0:\n",
    "        print(f\"feature {col} is a zero vector! Dropping it at train time, adding corresponding zeros in theta\")\n",
    "        zero_std_cols.append(col)\n",
    "synth_X_ohe.drop(columns = zero_std_cols, inplace = True)\n",
    "\n",
    "theta_aimsynth = public_linreg(synth_X_ohe, synth_y)\n",
    "for col in zero_std_cols:\n",
    "    theta_aimsynth.loc[col] = 0\n",
    "\n",
    "theta_aimsynth = theta_aimsynth.reindex(index=all_columns, fill_value=0)\n",
    "\n",
    "for i,col in enumerate(all_columns):\n",
    "    assert col == theta_aimsynth.index[i] \n",
    "\n",
    "theta_aimsynth = theta_aimsynth.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0c765e-1ccd-4b67-9374-5e108ce83d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_public = testLinReg(theta_public, X_test, y_test)\n",
    "mse_dpqueryss = testLinReg(theta_dpqueryss, X_test, y_test)\n",
    "mse_aimsynth = testLinReg(theta_aimsynth, X_test, y_test)\n",
    "mse_adassp = testLinReg(theta_adassp, X_test, y_test)\n",
    "\n",
    "print(f\"public mse: {mse_public}\")\n",
    "print(f\"dpqueryss mse: {mse_dpqueryss}\")\n",
    "print(f\"aimsynth mse: {mse_aimsynth}\")\n",
    "print(f\"adassp mse: {mse_adassp}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04cb3eba-4d3a-4195-b46c-2bac62630117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312a6f3c-945c-4e31-b1a4-a7a6163e56c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dpsynthesisml-pyenv310",
   "language": "python",
   "name": "dpsynthesisml-pyenv310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
